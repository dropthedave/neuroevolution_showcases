{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce0ec9d0",
   "metadata": {},
   "source": [
    "# Deliverable 2 - Evolving Neural Networks\n",
    "## Group Q\n",
    "\n",
    "Authors: \n",
    "\n",
    "- Rita Soares, 20220616\n",
    "- Cl√°udia Rocha, 20191249\n",
    "- Felix Gayer, 20220320\n",
    "- David Halder, 20220632\n",
    "- Leonhard Allgaier, 20220635\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dbdc32",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "1. Defining a string-based representation for the networks (the genotype).\n",
    "2. Defining a network class that is able to parse those instructions and build a functional pytorch network structure (the phenotype).\n",
    "3. Defining a string-based representation for the optimizer.\n",
    "4. Defining a way to parse those instructions and build and functional pytorch optimizer.\n",
    "5. Sample all parameter values from a grammar (this allows for a restricted search space and removes the need to deal with invalid combinations).\n",
    "6. Define 4 simple genetic operators: \n",
    "    - Network crossover\n",
    "    - Add layer mutation\n",
    "    - Remove layer mutation\n",
    "    - Change optimizer mutation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0365f16a",
   "metadata": {},
   "source": [
    "The grammar should contain a reasonable number of parameters for each of the layers/optimizers. Provided the values of the parameters are standard and within reason, there is no need to justify their choice.\n",
    "\n",
    "**The networks need to be able to use the following layers and their respective parameters:**\n",
    "\n",
    "- Linear : number of features, bias \n",
    "- BatchNorm1d : eps, momentum \n",
    "- LayerNorm : eps\n",
    "- Dropout : dropout probability (p) \n",
    "- AlphaDropout : dropout probability\n",
    "- [Sigmoid, ReLU, PReLU, ELU, SELU, GELU, CELU, SiLU]\n",
    "\n",
    "**The networks need to be able to be trained using the following optimizers and their respective parameters:** \n",
    "- Adam : lr, betas\n",
    "- AdamW : lr, betas, weight decay \n",
    "- Adadelta : lr, rho\n",
    "- NAdam : lr, betas, momentum decay \n",
    "- SGD : lr, momentum, nesterov\n",
    "\n",
    "**Each network needs to have at least 1 layer and a maximum 50 layers. This choice needs to be made at random when generating the genotype.**\n",
    "\n",
    "**The genetic operators that need to be implemented are the following:**\n",
    "- **Crossover** : Takes a subset of layers from 2 different networks and swaps them. This subset cannot include the first and final layers.\n",
    "- **Add layer mutation** : Add any new layer to any part of the genotype, and rebuild the network. It cannot be added before the first or after the last layer.\n",
    "- **Remove layer mutation**: Remove any layer that is neither the first nor the last from the genotype and rebuild the network.\n",
    "- **Change optimizer mutation**: Change any parameter from the optimizer genotype and rebuild it. If the type of optimizer is changed, the parameters must also be changed to ensure a valid optimizer is generated.\n",
    "\n",
    "### Experimental setup\n",
    "- Generate 5 random networks and train them. \n",
    "- Then, apply crossover to two of them and one of each mutation to the remaining 3 networks. Then, retrain the newly generated networks.\n",
    "- The networks should be trained on the mnist dataset, for 50 epochs, following the same procedure as used in the practical classes (showing the training and validation loss and accuracy per epoch).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "92463a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch\n",
    "#!pip install torchvision\n",
    "\n",
    "#!pip install mlxtend==0.19.0 -q\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets , transforms\n",
    "from dataclasses import dataclass\n",
    "import torchvision.transforms as T \n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "\n",
    "import torch.optim as optim\n",
    "import random "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ff3042",
   "metadata": {},
   "source": [
    "## 1. Defining a string-based representation for the networks (the genotype).\n",
    "### Generating a network genotype\n",
    "### Ensuring the networks have between 1 and 50 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "6100f7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulid_block(dict_network, in_features, out_features, drop_layer_rate, norm_layer_rate, out_features_fix=False,):\n",
    "    total_string = []\n",
    "\n",
    "    #### Add Linear layer \n",
    "    string = ''\n",
    "\n",
    "    string += ('Linear|')\n",
    "    \n",
    "    #If the ouput features are not fixed\n",
    "    if out_features_fix == False:\n",
    "        out_features = str(random.choice(dict_network['Linear']['in_features']))\n",
    "        \n",
    "    #add the parameters to the string\n",
    "    string += (in_features)\n",
    "    string += (',')\n",
    "    string += (out_features)\n",
    "    string += (',')\n",
    "    string += (str(random.choice(dict_network['Linear']['bias'])))\n",
    "    \n",
    "    #Append it to the genotype\n",
    "    total_string.append(string)\n",
    "\n",
    "    \n",
    "    if out_features_fix == False:\n",
    "        \n",
    "        if random.uniform(0, 1) < norm_layer_rate:\n",
    "            #### Add Normalization layer \n",
    "\n",
    "            string = ''\n",
    "\n",
    "            norm = random.choice(list(dict_network['Linear']['norm'].keys()))\n",
    "\n",
    "            string += norm\n",
    "            string += '|'\n",
    "            string += str(out_features)\n",
    "            string += ','\n",
    "\n",
    "            for i in list(dict_network['Linear']['norm'][norm].keys()):\n",
    "                string += str(random.choice(dict_network['Linear']['norm'][norm][i]))\n",
    "                string += ','\n",
    "\n",
    "            string = string[:-1]\n",
    "\n",
    "            total_string.append(string)\n",
    "\n",
    "\n",
    "        if random.uniform(0, 1) < drop_layer_rate:\n",
    "            #### Add Dropout layer \n",
    "\n",
    "            string = ''\n",
    "\n",
    "            drop = random.choice(list(dict_network['Linear']['drop'].keys()))\n",
    "\n",
    "            string += drop\n",
    "            string += '|'\n",
    "\n",
    "            for i in list(dict_network['Linear']['drop'][drop].keys()):\n",
    "                string += str(random.choice(dict_network['Linear']['drop'][drop][i]))\n",
    "                string += ','\n",
    "\n",
    "            string = string[:-1]\n",
    "\n",
    "            total_string.append(string)\n",
    "\n",
    "\n",
    "        #### Add Activation function \n",
    "\n",
    "        string = 'Act|'\n",
    "        string += random.choice(list(dict_network['Linear']['activation']))\n",
    "\n",
    "        total_string.append(string)\n",
    "\n",
    "    return total_string, out_features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def dict_to_string_representation_network(total_input_size, total_ouput_size, dict_network, min_amount_layers, max_amount_layers, drop_layer_rate=0.5, norm_layer_rate=0.5):\n",
    "    \n",
    "    if max_amount_layers > 50 or min_amount_layers <= 0:\n",
    "        print(\"Network size not sufficient, please choose between 1-50 layers\")\n",
    "        raise SystemExit\n",
    "    \n",
    "    total_input_size = str(total_input_size)\n",
    "    total_ouput_size = str(total_ouput_size)\n",
    "\n",
    "    total_string_representation = []\n",
    "    \n",
    "    numb_layers = random.randint(0,45)\n",
    "    counter = 0\n",
    "\n",
    "\n",
    "    #### Add layers\n",
    "\n",
    "    input_size = total_input_size\n",
    "\n",
    "    while counter < numb_layers:\n",
    "\n",
    "        block = bulid_block(dict_network, \n",
    "                            input_size, \n",
    "                            total_ouput_size,  \n",
    "                            drop_layer_rate, \n",
    "                            norm_layer_rate,\n",
    "                            out_features_fix=False)\n",
    "\n",
    "        for i in block[0]:\n",
    "            total_string_representation.append(i)\n",
    "\n",
    "        input_size = block[1]\n",
    "        counter += len(block[0])\n",
    "\n",
    "\n",
    "    #### Add output layer \n",
    "\n",
    "    block = bulid_block(dict_network, \n",
    "                        input_size, \n",
    "                        total_ouput_size,  \n",
    "                        drop_layer_rate, \n",
    "                        norm_layer_rate,\n",
    "                        out_features_fix=True)\n",
    "    \n",
    "    for i in block[0]:\n",
    "        total_string_representation.append(i)\n",
    "\n",
    "    return total_string_representation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df487a9a",
   "metadata": {},
   "source": [
    "### Defining the grammar (network)\n",
    "\n",
    "Parameter values were chosen around the default values\n",
    "\n",
    "Source:https://pytorch.org/docs/stable/nn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "60852bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_network = {\n",
    "    \n",
    "      \"Linear\" : {\n",
    "        \"in_features\" : [128,256,512],\n",
    "        \"out_features\" : [128,256,512],\n",
    "        \"bias\" : [True, False],\n",
    "        \"norm\" : {\n",
    "              \"BatchNorm1d\" : {\n",
    "                \"eps\" : [0.1, 0.01, 0.001],\n",
    "                \"momentum\" : [0.1, 0.5, 0.9] },\n",
    "\n",
    "              \"LayerNorm\" : {\n",
    "                \"eps\" : [1e-5, 1e-4, 1e-3]}\n",
    "                },\n",
    "\n",
    "        \"drop\" : {\n",
    "          \"Dropout\" : {\n",
    "            \"dropout_probability\" : [0.1, 0.3, 0.5, 0.7]},\n",
    "\n",
    "          \"AlphaDropout\" : {\n",
    "            \"dropout_probability\" : [0.1, 0.3, 0.5, 0.7]},\n",
    "        },\n",
    "\n",
    "        \"activation\" : [\"Sigmoid\", \"ReLU\", \"PReLU\", \"ELU\", \"SELU\", \"GELU\", \"CELU\", \"SiLU\"]\n",
    "\n",
    "    }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "e58e851c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Linear|784,128,False',\n",
       " 'LayerNorm|128,0.001',\n",
       " 'AlphaDropout|0.7',\n",
       " 'Act|ELU',\n",
       " 'Linear|128,256,True',\n",
       " 'BatchNorm1d|256,0.1,0.9',\n",
       " 'AlphaDropout|0.7',\n",
       " 'Act|Sigmoid',\n",
       " 'Linear|256,512,False',\n",
       " 'AlphaDropout|0.5',\n",
       " 'Act|PReLU',\n",
       " 'Linear|512,10,True']"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create the network string represetation \n",
    "\n",
    "total_input_size  = 28*28\n",
    "total_ouput_size  = 10\n",
    "dict_network      = dict_network\n",
    "min_amount_layers = 1\n",
    "max_amount_layers = 50\n",
    "drop_layer_rate   = 0.5\n",
    "norm_layer_rate   = 0.5\n",
    "\n",
    "string_model = dict_to_string_representation_network(total_input_size, \n",
    "                                                    total_ouput_size, \n",
    "                                                    dict_network, \n",
    "                                                    min_amount_layers, \n",
    "                                                    max_amount_layers,\n",
    "                                                    drop_layer_rate, \n",
    "                                                    norm_layer_rate)\n",
    "\n",
    "\n",
    "\n",
    "######## Vizualise a sample network string represetation\n",
    "\n",
    "string_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54536e12",
   "metadata": {},
   "source": [
    "## 2. Defining a network class that is able to parse those instructions and build a functional pytorch network structure (the phenotype).\n",
    "\n",
    "### Generating a network phenotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "3ee6abc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, layer_list):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for layer_str in layer_list:\n",
    "            layer = self.parse_layer_string(layer_str)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # to flatten the images into 1d arays\n",
    "        self.flat = nn.Flatten()\n",
    "\n",
    "\n",
    "    def parse_layer_string(self, layer_str):\n",
    "        \n",
    "        #Linear Layer\n",
    "        if layer_str.startswith('Linear'):\n",
    "            features = layer_str.split('|')[-1].split(',')\n",
    "            in_features = int(features[0])\n",
    "            out_features = int(features[1])\n",
    "            bias = bool(features[2])\n",
    "            return nn.Linear(in_features, out_features, bias)\n",
    "        \n",
    "        #Batch Normalisation\n",
    "        elif layer_str.startswith('Batch'):\n",
    "            features = layer_str.split('|')[-1].split(',')\n",
    "            num_features = int(features[0])\n",
    "            eps = float(features[1])\n",
    "            momentum = float(features[2])\n",
    "            return nn.BatchNorm1d(num_features, eps, momentum)\n",
    "        \n",
    "        #Layer Normalisation\n",
    "        elif layer_str.startswith('LayerNorm'):\n",
    "            features = layer_str.split('|')[-1].split(',')\n",
    "            normalized_shape = int(features[0])\n",
    "            eps = float(features[1])\n",
    "            return nn.LayerNorm(normalized_shape , eps)\n",
    "        \n",
    "        #Dropout Layer\n",
    "        elif layer_str.startswith('Dropout'):\n",
    "            p = float(layer_str.split('|')[-1])\n",
    "            return nn.Dropout(p=p)\n",
    "        \n",
    "        #Alpha Drouput Layer\n",
    "        elif layer_str.startswith('Alpha'):\n",
    "            p = float(layer_str.split('|')[-1])\n",
    "            return nn.AlphaDropout(p=p)\n",
    "        \n",
    "        #Activation Function\n",
    "        elif layer_str.startswith('Act'):\n",
    "            act = layer_str.split('|')[-1]\n",
    "            layer_class = getattr(nn,act)\n",
    "            return layer_class()\n",
    "        else:\n",
    "            raise ValueError('Invalid layer string: {}'.format(layer_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "7bca7938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): PReLU(num_parameters=1)\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): BatchNorm1d(512, eps=0.1, momentum=0.5, affine=True, track_running_stats=True)\n",
      "    (4): Dropout(p=0.3, inplace=False)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (7): LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
      "    (8): ELU(alpha=1.0)\n",
      "    (9): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (10): LayerNorm((512,), eps=0.0001, elementwise_affine=True)\n",
      "    (11): Dropout(p=0.5, inplace=False)\n",
      "    (12): SELU()\n",
      "    (13): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (14): AlphaDropout(p=0.1, inplace=False)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "### Create the network string represetation \n",
    "\n",
    "total_input_size  = 28*28\n",
    "total_ouput_size  = 10\n",
    "dict_network      = dict_network\n",
    "min_amount_layers = 1\n",
    "max_amount_layers = 50\n",
    "drop_layer_rate   = 0.5\n",
    "norm_layer_rate   = 0.5\n",
    "\n",
    "string_model = dict_to_string_representation_network(total_input_size, \n",
    "                                                    total_ouput_size, \n",
    "                                                    dict_network, \n",
    "                                                    min_amount_layers, \n",
    "                                                    max_amount_layers,\n",
    "                                                    drop_layer_rate, \n",
    "                                                    norm_layer_rate)\n",
    "\n",
    "\n",
    "### transfer the string represetation to a pytorch model \n",
    "\n",
    "model = Net(string_model)\n",
    "\n",
    "\n",
    "\n",
    "######## Vizualise a sample model \n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbb7762",
   "metadata": {},
   "source": [
    "## 3. Defining a string-based representation for the optimizer.\n",
    "### Defining the grammar for the optimizer\n",
    "\n",
    "To ensure a high degree of clarity and a clear structure of the notebook, the grammar for the optimiser and the network were defined separately.\n",
    "Parameter values were chosen around the default values\n",
    "Source: https://pytorch.org/docs/stable/optim.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "0ebf4f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dict_optimizer = {\n",
    "    \n",
    "    \"Adam\" : {\n",
    "        \"lr\" :           [0.1, 0.01, 0.001, 0.0001],\n",
    "        \"betas\" :        [(0.9, 0.999), (0.8, 0.9), (0.7, 0.99)] }, \n",
    "    \n",
    "    \"AdamW\" : {\n",
    "        \"lr\" :           [0.1, 0.01, 0.001, 0.0001],\n",
    "        \"betas\" :        [(0.9, 0.999), (0.8, 0.9), (0.7, 0.99)], \n",
    "        \"weight_decay\" : [0.01, 0.001, 0.0001]},\n",
    "    \n",
    "    \"Adadelta\" : {\n",
    "        \"lr\" :           [1.0, 0.1, 0.01, 0.001],\n",
    "        \"rho\" :          [0.9, 0.95, 0.99]},\n",
    "    \n",
    "    \"NAdam\" : {\n",
    "        \"lr\" :           [0.1, 0.01, 0.001, 0.0001],\n",
    "        \"betas\" :        [(0.9, 0.999), (0.8, 0.9), (0.7, 0.99)],\n",
    "        \"mom_decay\" :    [0.0001,0.001,0.01]},\n",
    "    \n",
    "    \"SGD\" : {\n",
    "        \"lr\" :           [0.1, 0.01, 0.001],\n",
    "        \"momentum\" :     [0.0, 0.9, 0.99],\n",
    "        \"nesterov\" :     [False, True]}\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92cad1a",
   "metadata": {},
   "source": [
    "### Generating an optimizer genotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "b0151eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_string_representation_optimizer(dict_optimizer):\n",
    "\n",
    "    string = ''\n",
    "\n",
    "    opt = random.choice(list(dict_optimizer.keys()))\n",
    "    string += opt\n",
    "    string += '|'\n",
    "\n",
    "    for i in list(dict_optimizer[opt].keys()):\n",
    "        string += str(random.choice(dict_optimizer[opt][i]))\n",
    "        string += ';'\n",
    "\n",
    "    string = string[:-1] \n",
    "    string = string.replace( '(', '')\n",
    "    string = string.replace( ')', '')\n",
    "\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "4d8a2466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adadelta|0.001;0.9\n",
      "Adadelta|0.1;0.95\n",
      "Adadelta|1.0;0.9\n",
      "Adam|0.001;0.7, 0.99\n",
      "AdamW|0.1;0.9, 0.999;0.0001\n"
     ]
    }
   ],
   "source": [
    "### Create the network string represetation \n",
    "\n",
    "total_input_size  = 28*28\n",
    "total_ouput_size  = 10\n",
    "dict_network      = dict_network\n",
    "min_amount_layers = 1\n",
    "max_amount_layers = 50\n",
    "drop_layer_rate   = 0.5\n",
    "norm_layer_rate   = 0.5\n",
    "\n",
    "string_model = dict_to_string_representation_network(total_input_size, \n",
    "                                                    total_ouput_size, \n",
    "                                                    dict_network, \n",
    "                                                    min_amount_layers, \n",
    "                                                    max_amount_layers,\n",
    "                                                    drop_layer_rate, \n",
    "                                                    norm_layer_rate)\n",
    "\n",
    "\n",
    "### transfer the string represetation to a pytorch model \n",
    "\n",
    "model = Net(string_model)\n",
    "\n",
    "\n",
    "### Create the optimizer string represetation \n",
    "\n",
    "opt_string = dict_to_string_representation_optimizer(dict_optimizer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######## Vizualise diffrent versions of the optimizer string represetation \n",
    "\n",
    "for _ in range(5):\n",
    "    print(dict_to_string_representation_optimizer(dict_optimizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e9ca03",
   "metadata": {},
   "source": [
    "## 4. Defining a way to parse those instructions and build and functional pytorch optimizer.\n",
    "\n",
    "#### Generating an optimizer phenotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "e5b22efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_opt_string(layer_str, model):\n",
    "    \n",
    "    #Parameters of the model\n",
    "    params =model.parameters()\n",
    "    \n",
    "    #AdamW (needs to be first so that it cannot be confused with Adam)\n",
    "    if layer_str.startswith('AdamW'):\n",
    "        hyp = layer_str.split('|')[-1].split(';')\n",
    "        lr = float(hyp[0])\n",
    "        betas = tuple(map(float, hyp[1].split(', ')))\n",
    "        weight_decay = float(hyp[2])\n",
    "        return optim.AdamW(params,lr, betas, weight_decay)\n",
    "    #Adam\n",
    "    elif layer_str.startswith('Adam'):\n",
    "        hyp = layer_str.split('|')[-1].split(';')\n",
    "        lr = float(hyp[0])\n",
    "        betas=tuple(map(float, hyp[1].split(', ')))\n",
    "        return optim.Adam(params, lr, betas)\n",
    "    #Adadelta\n",
    "    elif layer_str.startswith('Adadelta'):\n",
    "        hyp = layer_str.split('|')[-1].split(';')\n",
    "        lr = float(hyp[0])\n",
    "        rho = float(hyp[1])\n",
    "        return optim.Adadelta(params,lr, rho)\n",
    "    #NAdam\n",
    "    elif layer_str.startswith('NAdam'):\n",
    "        hyp = layer_str.split('|')[-1].split(';')\n",
    "        lr = float(hyp[0])\n",
    "        betas = tuple(map(float, hyp[1].split(', ')))\n",
    "        mom_decay = float(hyp[2])\n",
    "        return optim.NAdam(params, lr, betas, mom_decay)\n",
    "    #SGD\n",
    "    elif layer_str.startswith('SGD'):\n",
    "        hyp = layer_str.split('|')[-1].split(';')\n",
    "        lr = float(hyp[0])\n",
    "        momentum = float(hyp[1])\n",
    "        nesterov = bool(hyp[2])\n",
    "        return optim.SGD(params, lr, momentum, nesterov)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError('Invalid layer string: {}'.format(layer_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "32364064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: True\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0.99\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "### Create the network string represetation \n",
    "\n",
    "total_input_size  = 28*28\n",
    "total_ouput_size  = 10\n",
    "dict_network      = dict_network\n",
    "min_amount_layers = 1\n",
    "max_amount_layers = 50\n",
    "drop_layer_rate   = 0.5\n",
    "norm_layer_rate   = 0.5\n",
    "\n",
    "string_model = dict_to_string_representation_network(total_input_size, \n",
    "                                                    total_ouput_size, \n",
    "                                                    dict_network, \n",
    "                                                    min_amount_layers, \n",
    "                                                    max_amount_layers,\n",
    "                                                    drop_layer_rate, \n",
    "                                                    norm_layer_rate)\n",
    "\n",
    "\n",
    "### transfer the network string represetation to a pytorch model \n",
    "\n",
    "model = Net(string_model)\n",
    "\n",
    "\n",
    "### Create the optimizer string represetation \n",
    "\n",
    "opt_string = dict_to_string_representation_optimizer(dict_optimizer)\n",
    "\n",
    "\n",
    "### transfer the string represetation to a pytorch optimizer \n",
    "\n",
    "optimizer = parse_opt_string(opt_string, model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######## Vizualise a pytorch optimizer\n",
    "\n",
    "print(parse_opt_string(opt_string, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f1feee",
   "metadata": {},
   "source": [
    "## 5. Define 4 simple genetic operators\n",
    "### Implementing the genetic operators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13582000",
   "metadata": {},
   "source": [
    "### 5.1. Network crossover\n",
    "Takes a subset of layers from 2 different networks and swaps them. This subset cannot include the first and final layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "0a80cf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossover_adaption(offspring_1, point_1, point_2):\n",
    "        \n",
    "        #Get the previous layer type (which cointains input or output parameters)\n",
    "        pre_type = None\n",
    "        position = 0\n",
    "        while pre_type != (\"Linear\" or \"BatchNorm1d\" or \"LayerNorm\"):\n",
    "            position += 1\n",
    "            pre_type = offspring_1[point_1-position].split('|')[0]\n",
    "\n",
    "        #Calculate the position of the pre_layer \n",
    "        par_pos = point_1-position\n",
    "\n",
    "        #Check if the layer before is linear -> retrieve the output\n",
    "        if  pre_type == \"Linear\": \n",
    "            pre_output = offspring_1[par_pos].split('|')[1].split(\",\")[1]\n",
    "\n",
    "        #Check if the layer before is BatchNorm1D or LayerNorm -> retrieve the output\n",
    "        elif pre_type == \"BatchNorm1d\" or pre_type == \"LayerNorm\":\n",
    "            pre_output = offspring_1[par_pos].split('|')[1].split(\",\")[0]\n",
    "            \n",
    "    \n",
    "        #Get the next layer type (which cointains input or output parameters)\n",
    "        post_type = None\n",
    "        position_post = 0\n",
    "        while post_type !=(\"Linear\" or \"BatchNorm1d\" or \"LayerNorm\"):\n",
    "            position_post += 1\n",
    "            post_type = offspring_1[point_2+position_post].split('|')[0]  \n",
    "        \n",
    "        \n",
    "        #Calculate the position of the pre_layer \n",
    "        post_pos = point_2-position_post\n",
    "        #Get the input of that layer\n",
    "        post_input = offspring_1[post_pos].split('|')[1].split(\",\")[0]\n",
    "        \n",
    "        #Iterate through the crossover block to find out the position of the last linear layer\n",
    "        last_pos = point_2 + 1\n",
    "        type_backit = None\n",
    "        while type_backit != \"Linear\" and last_pos != (point_1-1):\n",
    "            last_pos = last_pos-1\n",
    "            type_backit=offspring_1[last_pos].split('|')[0] \n",
    "\n",
    "\n",
    "        #Iterate through the crossover block and change the input/output values \n",
    "        for bit in range(point_1, point_2):\n",
    "            #print(offspring_1[bit])\n",
    "            #Get the type of the current layer\n",
    "            bit_type = offspring_1[bit].split('|')[0]\n",
    "            \n",
    "            if bit_type == \"Linear\" and bit == last_pos:\n",
    "                params = offspring_1[bit].split('|')[1].split(\",\") #get the parameters of the layer\n",
    "                params[0] = pre_output #change the input\n",
    "                params[1] = post_input #change the output of the layer to the post output\n",
    "                params_str = ','.join(params) #make them a string again\n",
    "                layer_corrected = str(bit_type)+\"|\"+params_str #put the updated layer together\n",
    "                #print(\"linear\", layer_corrected)\n",
    "                offspring_1[bit] = layer_corrected #reintroduce it\n",
    "                \n",
    "            #if the layer is a linear layer -> change in and output\n",
    "            elif bit_type == \"Linear\": \n",
    "                params = offspring_1[bit].split('|')[1].split(\",\") #get the parameters of the layer\n",
    "                params[0] = pre_output #change the input\n",
    "                pre_output = params[1] #save the output\n",
    "                params_str = ','.join(params) #make them a string again\n",
    "                layer_corrected = str(bit_type)+\"|\"+params_str #put the updated layer together\n",
    "                #print(\"linear\", layer_corrected)\n",
    "                offspring_1[bit] = layer_corrected #reintroduce it\n",
    "\n",
    "            elif bit_type == \"BatchNorm1d\" or bit_type == \"LayerNorm\":\n",
    "                params = offspring_1[bit].split('|')[1].split(\",\") #get the parameters of the layer\n",
    "                params[0] = pre_output #change the input\n",
    "                params_str = ','.join(params) #make them a string again\n",
    "                layer_corrected = str(bit_type)+\"|\"+params_str #put the updated layer together\n",
    "                #print(\"norm:\",layer_corrected)\n",
    "                offspring_1[bit] = layer_corrected #reintroduce it\n",
    "            \n",
    "            #else:\n",
    "             #   print(\"others\", offspring_1[bit])\n",
    "                \n",
    "        return offspring_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "7c27610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossover(layer_string_network_1, layer_string_network_2):\n",
    "    \n",
    "    if len(layer_string_network_1) == 1 or  len(layer_string_network_2) == 1:\n",
    "        print(\"Network too small, please generate a new one\")\n",
    "        raise SystemExit\n",
    "\n",
    "    # To ensure that the two splitting points are not the same number we initialise both points with \"1\"\n",
    "    # and loop through the random selection until a crossover block is chosen \n",
    "    point_1 = None\n",
    "    point_2 = None\n",
    "    \n",
    "    #Get a copy of the networks\n",
    "    offspring_1 = layer_string_network_1.copy()\n",
    "    offspring_2 = layer_string_network_2.copy()   \n",
    "\n",
    "    #The smaller network provides the length of the block\n",
    "    if len(layer_string_network_1) <= len(layer_string_network_2):\n",
    "        \n",
    "        #Try until the points are not the same\n",
    "        while point_1 == point_2:\n",
    "            point_1 = np.random.randint(1,len(layer_string_network_1)-1)\n",
    "            point_2 = np.random.randint(point_1+1,len(layer_string_network_1))\n",
    "\n",
    "        #Conduct the crossover\n",
    "        offspring_1[point_1:point_2] = layer_string_network_2[point_1:point_2]\n",
    "        offspring_2[point_1:point_2] = layer_string_network_1[point_1:point_2]\n",
    "        \n",
    "    #The smaller network provides the length of the block\n",
    "    elif len(layer_string_network_1) > len(layer_string_network_2):\n",
    "        \n",
    "        #Try until the points are not the same\n",
    "        while point_1 == point_2:\n",
    "            point_1 = np.random.randint(1,len(layer_string_network_2)-1)\n",
    "            point_2 = np.random.randint(point_1+1,len(layer_string_network_2))\n",
    "        \n",
    "        #Conduct the crossover\n",
    "        offspring_1[point_1:point_2] = layer_string_network_2[point_1:point_2]\n",
    "        offspring_2[point_1:point_2] = layer_string_network_1[point_1:point_2]\n",
    "        \n",
    "\n",
    "        # Now we need to ensure that the network is actually functional \n",
    "        # -> input and output parameters need to be adapted             \n",
    "    \n",
    "    result_1 = crossover_adaption(offspring_1, point_1, point_2)\n",
    "    result_2 = crossover_adaption(offspring_2, point_1, point_2)\n",
    "    \n",
    "    return result_1, result_2\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "fa49e28e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Linear|784,512,False',\n",
       "  'Act|PReLU',\n",
       "  'Linear|512,128,False',\n",
       "  'BatchNorm1d|128,0.1,0.5',\n",
       "  'AlphaDropout|0.3',\n",
       "  'Linear|128,128,False',\n",
       "  'BatchNorm1d|128,0.01,0.9',\n",
       "  'AlphaDropout|0.1',\n",
       "  'Act|ELU',\n",
       "  'Linear|256,512,False',\n",
       "  'Act|ReLU',\n",
       "  'Linear|512,10,True'],\n",
       " ['Linear|784,256,False',\n",
       "  'Act|Sigmoid',\n",
       "  'Linear|256,128,False',\n",
       "  'BatchNorm1d|128,0.01,0.9',\n",
       "  'Act|CELU',\n",
       "  'Act|SiLU',\n",
       "  'Linear|128,128,False',\n",
       "  'Act|PReLU',\n",
       "  'Linear|256,256,True',\n",
       "  'BatchNorm1d|256,0.01,0.5',\n",
       "  'Act|PReLU',\n",
       "  'Linear|256,128,True',\n",
       "  'Dropout|0.5',\n",
       "  'Act|ELU',\n",
       "  'Linear|128,256,False',\n",
       "  'Dropout|0.1',\n",
       "  'Act|SELU',\n",
       "  'Linear|256,128,False',\n",
       "  'Act|SELU',\n",
       "  'Linear|128,128,False',\n",
       "  'LayerNorm|128,0.001',\n",
       "  'Act|Sigmoid',\n",
       "  'Linear|128,10,True'])"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create two network string represetation to crossover\n",
    "\n",
    "total_input_size  = 28*28\n",
    "total_ouput_size  = 10\n",
    "dict_network      = dict_network\n",
    "min_amount_layers = 1\n",
    "max_amount_layers = 50\n",
    "drop_layer_rate   = 0.5\n",
    "norm_layer_rate   = 0.5\n",
    "\n",
    "string_model_1 = dict_to_string_representation_network(total_input_size, \n",
    "                                                    total_ouput_size, \n",
    "                                                    dict_network, \n",
    "                                                    min_amount_layers, \n",
    "                                                    max_amount_layers,\n",
    "                                                    drop_layer_rate, \n",
    "                                                    norm_layer_rate)\n",
    "\n",
    "string_model_2 = dict_to_string_representation_network(total_input_size, \n",
    "                                                    total_ouput_size, \n",
    "                                                    dict_network, \n",
    "                                                    min_amount_layers, \n",
    "                                                    max_amount_layers,\n",
    "                                                    drop_layer_rate, \n",
    "                                                    norm_layer_rate)\n",
    "\n",
    "#Cross Them\n",
    "crossed_1, crossed_2=crossover(string_model_1, string_model_2)\n",
    "\n",
    "#Visualise it: \n",
    "\n",
    "crossover(string_model_1, string_model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "7db2d95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True\n",
      "True True\n"
     ]
    }
   ],
   "source": [
    "#test the crossover -> \n",
    "print(string_model_1[-1] ==crossed_1[-1],string_model_1[0] ==crossed_1[0] )\n",
    "print(string_model_2[-1] ==crossed_2[-1], string_model_2[0] ==crossed_2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b3534d",
   "metadata": {},
   "source": [
    "### 5.2. Add layer mutation\n",
    "Add any new layer to any part of the genotype, and rebuild the network. It cannot be added before the first or after the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "80a01fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_layer_mutation(layer_string_network, dict_network):     \n",
    "    \n",
    "    #if the network is too small: throw an error\n",
    "    if len(layer_string_network) == 1:\n",
    "        print(\"Network too small, please generate a new one\")\n",
    "        raise SystemExit\n",
    "        \n",
    "    #Choose position of the layer which will be added\n",
    "    mut_position = np.random.randint(1,len(layer_string_network)-1) #-1 to not choose the last layer\n",
    "    offspring = layer_string_network.copy()\n",
    "    \n",
    "    \n",
    "    #generate a model to retrieve a random layer \n",
    "    #length of the Mut Model\n",
    "    mut_length = 1\n",
    "    while mut_length < 3:\n",
    "        mutation_model = dict_to_string_representation_network(\n",
    "                                                    total_input_size, \n",
    "                                                    total_ouput_size, \n",
    "                                                    dict_network, \n",
    "                                                    min_amount_layers, \n",
    "                                                    max_amount_layers,\n",
    "                                                    drop_layer_rate, \n",
    "                                                    norm_layer_rate)\n",
    "        mut_length=len(mutation_model)\n",
    "        print(\"Mutation Length\", mut_length)\n",
    "    \n",
    "    rand_mut_layer = mutation_model[np.random.randint(1,len(mutation_model)-1)]\n",
    "    mut_l_type = rand_mut_layer.split('|')[0]\n",
    "    \n",
    "    #put the new layer in\n",
    "    offspring.insert(mut_position, rand_mut_layer)\n",
    "    \n",
    "    #Check if the mutation is a dropout or activation layer -> no action\n",
    "    if mut_l_type == \"Act\" or mut_l_type == \"Dropout\" or mut_l_type == \"AlphaDropout\":\n",
    "        print(\"No Action needed\")\n",
    "    \n",
    "    #Check if the mutation layer is linear -> input and output must be adapted\n",
    "    elif mut_l_type == \"Linear\":\n",
    "        \n",
    "        pre_type = None\n",
    "        position = 0\n",
    "        #Check if the layer before is linear -> make the output the input of the new layer\n",
    "        while pre_type != (\"Linear\" or \"BatchNorm1d\" or \"LayerNorm\"):\n",
    "            position += 1\n",
    "            pre_type = offspring[mut_position-position].split('|')[0]\n",
    "            \n",
    "        pre_pos = mut_position - position\n",
    "        if  pre_type == \"Linear\": \n",
    "            pre_output = offspring[pre_pos].split('|')[1].split(\",\")[1]\n",
    "            params = rand_mut_layer.split('|')[1].split(\",\")\n",
    "            params[0] = pre_output\n",
    "            params_str = ','.join(params)\n",
    "            rand_mut_layer = str(mut_l_type)+\"|\"+params_str #change rand_mut_layer so it can be used later\n",
    "        \n",
    "        elif pre_type == \"BatchNorm1d\" or pre_type == \"LayerNorm\":\n",
    "            pre_output = offspring[pre_pos].split('|')[1].split(\",\")[0]\n",
    "            params = rand_mut_layer.split('|')[1].split(\",\")\n",
    "            params[0] = pre_output\n",
    "            params_str = ','.join(params)\n",
    "            rand_mut_layer = str(mut_l_type)+\"|\"+params_str #change rand_mut_layer so it can be used later\n",
    "        \n",
    "        #Check if the layer after is linear -> adapt the output of the new layer \n",
    "        \n",
    "        post_type = None\n",
    "        position_post = 0\n",
    "        while post_type !=(\"Linear\" or \"BatchNorm1d\" or \"LayerNorm\"):\n",
    "            position_post += 1\n",
    "            post_type = offspring[mut_position+position_post].split('|')[0] \n",
    "        post_pos = mut_position+position_post\n",
    "        if post_type == \"Linear\" or post_type == \"BatchNorm1d\" or post_type == \"LayerNorm\":\n",
    "                post_input = offspring[post_pos].split('|')[1].split(\",\")[0]\n",
    "                params = rand_mut_layer.split('|')[1].split(\",\")\n",
    "                params[1] = post_input\n",
    "                params_str = ','.join(params)\n",
    "                rand_mut_layer = str(mut_l_type)+\"|\"+params_str #change rand_mut_layer so it can be used later\n",
    "                \n",
    "                \n",
    "    #If the mutation layer is norm -> check layer before \n",
    "    elif mut_l_type == \"BatchNorm1d\" or mut_l_type == \"LayerNorm\":\n",
    "        pre_type = None\n",
    "        position = 0\n",
    "        #Check if the layer before is linear -> make the output the input of the new layer\n",
    "        while pre_type != (\"Linear\" or \"BatchNorm1d\" or \"LayerNorm\"):\n",
    "            position += 1\n",
    "            pre_type = offspring[mut_position-position].split('|')[0]\n",
    "            \n",
    "        pre_pos = mut_position - position\n",
    "        if  pre_type == \"Linear\": \n",
    "            pre_output = offspring[pre_pos].split('|')[1].split(\",\")[1]\n",
    "            params = rand_mut_layer.split('|')[1].split(\",\")\n",
    "            params[0] = pre_output\n",
    "            params_str = ','.join(params)\n",
    "            rand_mut_layer = str(mut_l_type)+\"|\"+params_str #change rand_mut_layer so it can be used later\n",
    "            \n",
    "        elif pre_type == \"BatchNorm1d\" or pre_type == \"LayerNorm\":\n",
    "            pre_output = offspring[pre_pos].split('|')[1].split(\",\")[0]\n",
    "            params = rand_mut_layer.split('|')[1].split(\",\")\n",
    "            params[0] = pre_output\n",
    "            params_str = ','.join(params)\n",
    "            rand_mut_layer = str(mut_l_type)+\"|\"+params_str #change rand_mut_layer so it can be used later\n",
    "            \n",
    "                \n",
    "    offspring[mut_position]= rand_mut_layer\n",
    "    print(mut_position)\n",
    "    print(\"Integrity Check\")\n",
    "    print(offspring[mut_position-1])\n",
    "    print(offspring[mut_position])\n",
    "    print(offspring[mut_position+1])\n",
    "    return offspring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "57b93930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutation Length 11\n",
      "No Action needed\n",
      "24\n",
      "Integrity Check\n",
      "Act|CELU\n",
      "Act|GELU\n",
      "Linear|256,512,True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Linear|784,512,False',\n",
       " 'LayerNorm|512,1e-05',\n",
       " 'Act|ReLU',\n",
       " 'Linear|512,256,False',\n",
       " 'Dropout|0.1',\n",
       " 'Act|ELU',\n",
       " 'Linear|256,512,True',\n",
       " 'LayerNorm|512,0.0001',\n",
       " 'Act|ReLU',\n",
       " 'Linear|512,128,False',\n",
       " 'BatchNorm1d|128,0.01,0.9',\n",
       " 'AlphaDropout|0.7',\n",
       " 'Act|SiLU',\n",
       " 'Linear|128,512,False',\n",
       " 'AlphaDropout|0.7',\n",
       " 'Act|CELU',\n",
       " 'Linear|512,512,True',\n",
       " 'Dropout|0.3',\n",
       " 'Act|GELU',\n",
       " 'Linear|512,128,True',\n",
       " 'BatchNorm1d|128,0.001,0.5',\n",
       " 'Act|ELU',\n",
       " 'Linear|128,256,True',\n",
       " 'Act|CELU',\n",
       " 'Act|GELU',\n",
       " 'Linear|256,512,True',\n",
       " 'LayerNorm|512,0.001',\n",
       " 'Act|ReLU',\n",
       " 'Linear|512,128,True',\n",
       " 'AlphaDropout|0.7',\n",
       " 'Act|SiLU',\n",
       " 'Linear|128,256,False',\n",
       " 'LayerNorm|256,0.0001',\n",
       " 'Act|CELU',\n",
       " 'Linear|256,256,True',\n",
       " 'LayerNorm|256,1e-05',\n",
       " 'Act|SiLU',\n",
       " 'Linear|256,128,False',\n",
       " 'BatchNorm1d|128,0.1,0.9',\n",
       " 'AlphaDropout|0.7',\n",
       " 'Act|SELU',\n",
       " 'Linear|128,128,True',\n",
       " 'Dropout|0.1',\n",
       " 'Act|CELU',\n",
       " 'Linear|128,512,False',\n",
       " 'Act|ReLU',\n",
       " 'Linear|512,10,False']"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create the network string represetation \n",
    "\n",
    "total_input_size  = 28*28\n",
    "total_ouput_size  = 10\n",
    "dict_network      = dict_network\n",
    "min_amount_layers = 1\n",
    "max_amount_layers = 50\n",
    "drop_layer_rate   = 0.5\n",
    "norm_layer_rate   = 0.5\n",
    "\n",
    "string_model = dict_to_string_representation_network(total_input_size, \n",
    "                                                    total_ouput_size, \n",
    "                                                    dict_network, \n",
    "                                                    min_amount_layers, \n",
    "                                                    max_amount_layers,\n",
    "                                                    drop_layer_rate, \n",
    "                                                    norm_layer_rate)\n",
    "\n",
    "\n",
    "######## Vizualise the network with extra layer\n",
    "\n",
    "test_model=add_layer_mutation(string_model, dict_network)\n",
    "test_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8ad37e",
   "metadata": {},
   "source": [
    "### 5. 3. Remove layer mutation\n",
    "Remove any layer that is neither the first nor the last from the genotype and rebuild the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "65037257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutation_remove_layer(layer_string_network):\n",
    "    \n",
    "    if len(layer_string_network) == 1:\n",
    "        print(\"Network too small, please generate a new one\")\n",
    "        raise SystemExit\n",
    "        \n",
    "    #Choose a random layer from each network\n",
    "    rand_layer_type = \"Linear\"\n",
    "    while rand_layer_type == \"Linear\":\n",
    "        rand_layer_1 = np.random.randint(1,len(layer_string_network)-1)\n",
    "        rand_layer_type = layer_string_network[rand_layer_1].split('|')[0]\n",
    "    print('removed Layer:', rand_layer_1, layer_string_network[rand_layer_1])\n",
    "    #remove the layer and return the new genotype\n",
    "    offspring =layer_string_network.copy()\n",
    "    offspring.pop(rand_layer_1)\n",
    "    return offspring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "87613c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed Layer: 7 BatchNorm1d|512,0.1,0.9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Linear|784,256,False',\n",
       " 'LayerNorm|256,0.0001',\n",
       " 'Act|ELU',\n",
       " 'Linear|256,512,False',\n",
       " 'Dropout|0.5',\n",
       " 'Act|Sigmoid',\n",
       " 'Linear|512,512,False',\n",
       " 'Act|Sigmoid',\n",
       " 'Linear|512,512,False',\n",
       " 'Dropout|0.7',\n",
       " 'Act|GELU',\n",
       " 'Linear|512,128,False',\n",
       " 'Dropout|0.5',\n",
       " 'Act|Sigmoid',\n",
       " 'Linear|128,128,False',\n",
       " 'BatchNorm1d|128,0.1,0.9',\n",
       " 'AlphaDropout|0.7',\n",
       " 'Act|ReLU',\n",
       " 'Linear|128,512,True',\n",
       " 'LayerNorm|512,1e-05',\n",
       " 'Dropout|0.3',\n",
       " 'Act|GELU',\n",
       " 'Linear|512,10,True']"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create the network string represetation \n",
    "\n",
    "total_input_size  = 28*28\n",
    "total_ouput_size  = 10\n",
    "dict_network      = dict_network\n",
    "min_amount_layers = 1\n",
    "max_amount_layers = 50\n",
    "drop_layer_rate   = 0.5\n",
    "norm_layer_rate   = 0.5\n",
    "\n",
    "string_model = dict_to_string_representation_network(total_input_size, \n",
    "                                                    total_ouput_size, \n",
    "                                                    dict_network, \n",
    "                                                    min_amount_layers, \n",
    "                                                    max_amount_layers,\n",
    "                                                    drop_layer_rate, \n",
    "                                                    norm_layer_rate)\n",
    "\n",
    "\n",
    "######## Vizualise the orignal network string represetation \n",
    "\n",
    "string_model\n",
    "mutation_remove_layer(string_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "f0a33cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed Layer: 20 LayerNorm|512,1e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Linear|784,256,False',\n",
       " 'LayerNorm|256,0.0001',\n",
       " 'Act|ELU',\n",
       " 'Linear|256,512,False',\n",
       " 'Dropout|0.5',\n",
       " 'Act|Sigmoid',\n",
       " 'Linear|512,512,False',\n",
       " 'BatchNorm1d|512,0.1,0.9',\n",
       " 'Act|Sigmoid',\n",
       " 'Linear|512,512,False',\n",
       " 'Dropout|0.7',\n",
       " 'Act|GELU',\n",
       " 'Linear|512,128,False',\n",
       " 'Dropout|0.5',\n",
       " 'Act|Sigmoid',\n",
       " 'Linear|128,128,False',\n",
       " 'BatchNorm1d|128,0.1,0.9',\n",
       " 'AlphaDropout|0.7',\n",
       " 'Act|ReLU',\n",
       " 'Linear|128,512,True',\n",
       " 'Dropout|0.3',\n",
       " 'Act|GELU',\n",
       " 'Linear|512,10,True']"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######## Vizualise the mutated network string represetation \n",
    "\n",
    "mutation_remove_layer(string_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867a840a",
   "metadata": {},
   "source": [
    "### 5.4. Change optimizer mutation\n",
    "Change any parameter from the optimizer genotype and rebuild it. If the type of optimizer is changed, the parameters must also be changed to ensure a valid optimizer is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "c0bc1fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutation_change_opt(opt_string, dict_optimizer=dict_optimizer):\n",
    "    parameters = []  # List to store the parameters\n",
    "    \n",
    "    # Unbuild the string\n",
    "    string = opt_string.split(\"|\")  # Split the string by '|'\n",
    "    parameters.append(string[0])  # Add the first part to parameters list\n",
    "    string = string[1].split(\";\")  # Split the remaining part by ';'\n",
    "    \n",
    "    # Iterate through the parts and add them to parameters list\n",
    "    for i in string:\n",
    "        if ',' in i:\n",
    "            parameters.append('('+(i)+')')\n",
    "        else:\n",
    "            parameters.append(i)\n",
    "    \n",
    "    # Select the parameter to change\n",
    "    select_index = np.random.randint(0, len(parameters))  # Randomly select an index\n",
    "    x = str(parameters[select_index])  # Get the selected parameter\n",
    "    \n",
    "    # Print original information\n",
    "    print('original optimizer string:', opt_string)\n",
    "    print('parameters in string', parameters)\n",
    "    print('parameter to change:', x)\n",
    "    \n",
    "    if select_index == 0:\n",
    "        param = list(dict_optimizer.keys())\n",
    "        print('out of following parameters:', param)\n",
    "        \n",
    "        # Choose a new parameter until it's different from the current one\n",
    "        while parameters[select_index] == x:\n",
    "            new_string = dict_to_string_representation_optimizer(dict_optimizer)\n",
    "            x = new_string.split(\"|\")[0]\n",
    "        \n",
    "        print('new parameter:', x)\n",
    "        return new_string\n",
    "\n",
    "    param_keys = list(dict_optimizer[parameters[0]].keys())\n",
    "\n",
    "    if select_index == 1:\n",
    "        param = dict_optimizer[parameters[0]][param_keys[0]]\n",
    "\n",
    "    if select_index == 2:\n",
    "        param = dict_optimizer[parameters[0]][param_keys[1]]   \n",
    "\n",
    "    if select_index == 3:\n",
    "        param = dict_optimizer[parameters[0]][param_keys[2]]\n",
    "        \n",
    "    print('out of following parameters:', param)\n",
    "\n",
    "    # Choose a new parameter until it's different from the current one\n",
    "    while parameters[select_index] == x:\n",
    "        x = str(random.choice(param))\n",
    "        \n",
    "    print('new parameter:', x)\n",
    "    \n",
    "    # Rebuild the string\n",
    "    parameters[select_index] = x\n",
    "    \n",
    "    string = ''\n",
    "    opt = parameters[0]\n",
    "    string += opt\n",
    "    string += '|'\n",
    "    \n",
    "    # Iterate through parameters and add them to the string\n",
    "    for i in parameters[1:]:\n",
    "        string += i\n",
    "        string += ';'\n",
    "\n",
    "    string = string[:-1] \n",
    "    string = string.replace('(', '')\n",
    "    string = string.replace(')', '')\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "56193a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original optimizer string: SGD|0.1;0.0;False\n",
      "parameters in string ['SGD', '0.1', '0.0', 'False']\n",
      "parameter to change: 0.1\n",
      "out of following parameters: [0.1, 0.01, 0.001]\n",
      "new parameter: 0.01\n",
      "SGD|0.01;0.0;False\n",
      "-----\n",
      "original optimizer string: SGD|0.01;0.9;False\n",
      "parameters in string ['SGD', '0.01', '0.9', 'False']\n",
      "parameter to change: 0.01\n",
      "out of following parameters: [0.1, 0.01, 0.001]\n",
      "new parameter: 0.1\n",
      "SGD|0.1;0.9;False\n",
      "-----\n",
      "original optimizer string: SGD|0.001;0.9;True\n",
      "parameters in string ['SGD', '0.001', '0.9', 'True']\n",
      "parameter to change: SGD\n",
      "out of following parameters: ['Adam', 'AdamW', 'Adadelta', 'NAdam', 'SGD']\n",
      "new parameter: Adadelta\n",
      "Adadelta|0.01;0.99\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "######## Vizualise the change optimizer mutation\n",
    "\n",
    "for _ in range(3):\n",
    "\n",
    "    opt_string = dict_to_string_representation_optimizer(dict_optimizer)\n",
    "\n",
    "    test = mutation_change_opt(opt_string)\n",
    "    print(test)\n",
    "    print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d0d41e",
   "metadata": {},
   "source": [
    "## 6. Experimental setup\n",
    "\n",
    "- Generate 5 random networks and train them. \n",
    "- Then, apply crossover to two of them and one of each mutation to the remaining 3 networks. Then, retrain the newly generated networks.\n",
    "- The networks should be trained on the mnist dataset, for 50 epochs, following the same procedure as used in the practical classes (showing the training and validation loss and accuracy per epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "d73f774c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', \n",
    "                               train=True, \n",
    "                               download=True, \n",
    "                               transform=transforms.ToTensor())\n",
    "\n",
    "validation_dataset = datasets.MNIST('./data', \n",
    "                                    train=False, \n",
    "                                    transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=True)\n",
    "\n",
    "validation_loader = DataLoader(dataset=validation_dataset, \n",
    "                                batch_size=batch_size, \n",
    "                                shuffle=False)\n",
    "#Epochs 50 -> for all experiments\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "39c2f897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_validation(model, train_loader, validation_loader, loss_fn, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        accuracy_hist_train = 0\n",
    "        accuracy_hist_val = 0  # Initialize validation accuracy\n",
    "        loss_hist_train = 0\n",
    "        loss_hist_val = 0  # Initialize validation loss\n",
    "\n",
    "        # Training phase\n",
    "        model.train()  # Set the model to train mode\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            pred = model(x_batch)\n",
    "            loss = loss_fn(pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            is_correct = (torch.argmax(pred, dim=1) == y_batch).float()\n",
    "            accuracy_hist_train += is_correct.sum()\n",
    "            loss_hist_train += loss.item()\n",
    "        accuracy_hist_train /= len(train_loader.dataset)\n",
    "        loss_hist_train /= len(train_loader)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        with torch.no_grad():  # Disable gradient calculation\n",
    "            for x_batch, y_batch in validation_loader:\n",
    "                pred = model(x_batch)\n",
    "                is_correct = (torch.argmax(pred, dim=1) == y_batch).float()\n",
    "                accuracy_hist_val += is_correct.sum()\n",
    "                loss = loss_fn(pred, y_batch)\n",
    "                loss_hist_val += loss.item()\n",
    "            accuracy_hist_val /= len(validation_loader.dataset)\n",
    "            loss_hist_val /= len(validation_loader)\n",
    "\n",
    "        print(f'Epoch {epoch}  Training Accuracy: {accuracy_hist_train:.4f}  Training Loss: {loss_hist_train:.4f}  Validation Accuracy: {accuracy_hist_val:.4f}  Validation Loss: {loss_hist_val:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8405d4",
   "metadata": {},
   "source": [
    "### 6.1 Experiment 1 - Network 1 & Network 2 // Crossover "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a97e81",
   "metadata": {},
   "source": [
    "#### 6.1.1 Train Network 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "e3d92aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): BatchNorm1d(512, eps=0.1, momentum=0.5, affine=True, track_running_stats=True)\n",
      "    (2): AlphaDropout(p=0.1, inplace=False)\n",
      "    (3): Sigmoid()\n",
      "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (5): LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
      "    (6): Dropout(p=0.7, inplace=False)\n",
      "    (7): GELU()\n",
      "    (8): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (9): ELU(alpha=1.0)\n",
      "    (10): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (11): LayerNorm((128,), eps=0.0001, elementwise_affine=True)\n",
      "    (12): Dropout(p=0.1, inplace=False)\n",
      "    (13): ELU(alpha=1.0)\n",
      "    (14): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (15): SELU()\n",
      "    (16): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
      ")\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.8, 0.9)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0  Training Accuracy: 0.7646  Training Loss: 0.7343  Validation Accuracy: 0.9114  Validation Loss: 0.3063\n",
      "Epoch 1  Training Accuracy: 0.8655  Training Loss: 0.4340  Validation Accuracy: 0.9203  Validation Loss: 0.2749\n",
      "Epoch 2  Training Accuracy: 0.8863  Training Loss: 0.3719  Validation Accuracy: 0.9345  Validation Loss: 0.2254\n",
      "Epoch 3  Training Accuracy: 0.8958  Training Loss: 0.3367  Validation Accuracy: 0.9286  Validation Loss: 0.2271\n",
      "Epoch 4  Training Accuracy: 0.9043  Training Loss: 0.3082  Validation Accuracy: 0.9430  Validation Loss: 0.1906\n",
      "Epoch 5  Training Accuracy: 0.9102  Training Loss: 0.2929  Validation Accuracy: 0.9444  Validation Loss: 0.1807\n",
      "Epoch 6  Training Accuracy: 0.9162  Training Loss: 0.2766  Validation Accuracy: 0.9481  Validation Loss: 0.1779\n",
      "Epoch 7  Training Accuracy: 0.9184  Training Loss: 0.2621  Validation Accuracy: 0.9494  Validation Loss: 0.1671\n",
      "Epoch 8  Training Accuracy: 0.9228  Training Loss: 0.2521  Validation Accuracy: 0.9479  Validation Loss: 0.1769\n",
      "Epoch 9  Training Accuracy: 0.9275  Training Loss: 0.2386  Validation Accuracy: 0.9532  Validation Loss: 0.1538\n",
      "Epoch 10  Training Accuracy: 0.9302  Training Loss: 0.2278  Validation Accuracy: 0.9567  Validation Loss: 0.1468\n",
      "Epoch 11  Training Accuracy: 0.9332  Training Loss: 0.2168  Validation Accuracy: 0.9532  Validation Loss: 0.1532\n",
      "Epoch 12  Training Accuracy: 0.9370  Training Loss: 0.2054  Validation Accuracy: 0.9452  Validation Loss: 0.1799\n",
      "Epoch 13  Training Accuracy: 0.9401  Training Loss: 0.1923  Validation Accuracy: 0.9450  Validation Loss: 0.1873\n",
      "Epoch 14  Training Accuracy: 0.9413  Training Loss: 0.1899  Validation Accuracy: 0.9439  Validation Loss: 0.1944\n",
      "Epoch 15  Training Accuracy: 0.9434  Training Loss: 0.1851  Validation Accuracy: 0.9483  Validation Loss: 0.1723\n",
      "Epoch 16  Training Accuracy: 0.9453  Training Loss: 0.1790  Validation Accuracy: 0.9480  Validation Loss: 0.1718\n",
      "Epoch 17  Training Accuracy: 0.9470  Training Loss: 0.1724  Validation Accuracy: 0.9440  Validation Loss: 0.1849\n",
      "Epoch 18  Training Accuracy: 0.9494  Training Loss: 0.1668  Validation Accuracy: 0.9442  Validation Loss: 0.1926\n",
      "Epoch 19  Training Accuracy: 0.9512  Training Loss: 0.1613  Validation Accuracy: 0.9518  Validation Loss: 0.1623\n",
      "Epoch 20  Training Accuracy: 0.9517  Training Loss: 0.1554  Validation Accuracy: 0.9449  Validation Loss: 0.1889\n",
      "Epoch 21  Training Accuracy: 0.9523  Training Loss: 0.1552  Validation Accuracy: 0.9466  Validation Loss: 0.1862\n",
      "Epoch 22  Training Accuracy: 0.9544  Training Loss: 0.1486  Validation Accuracy: 0.9438  Validation Loss: 0.1884\n",
      "Epoch 23  Training Accuracy: 0.9545  Training Loss: 0.1469  Validation Accuracy: 0.9468  Validation Loss: 0.1806\n",
      "Epoch 24  Training Accuracy: 0.9553  Training Loss: 0.1433  Validation Accuracy: 0.9373  Validation Loss: 0.2102\n",
      "Epoch 25  Training Accuracy: 0.9568  Training Loss: 0.1403  Validation Accuracy: 0.9478  Validation Loss: 0.1780\n",
      "Epoch 26  Training Accuracy: 0.9574  Training Loss: 0.1358  Validation Accuracy: 0.9514  Validation Loss: 0.1602\n",
      "Epoch 27  Training Accuracy: 0.9589  Training Loss: 0.1340  Validation Accuracy: 0.9351  Validation Loss: 0.2253\n",
      "Epoch 28  Training Accuracy: 0.9584  Training Loss: 0.1340  Validation Accuracy: 0.9433  Validation Loss: 0.1955\n",
      "Epoch 29  Training Accuracy: 0.9597  Training Loss: 0.1302  Validation Accuracy: 0.9388  Validation Loss: 0.2133\n",
      "Epoch 30  Training Accuracy: 0.9621  Training Loss: 0.1253  Validation Accuracy: 0.9471  Validation Loss: 0.1792\n",
      "Epoch 31  Training Accuracy: 0.9609  Training Loss: 0.1242  Validation Accuracy: 0.9533  Validation Loss: 0.1623\n",
      "Epoch 32  Training Accuracy: 0.9615  Training Loss: 0.1235  Validation Accuracy: 0.9442  Validation Loss: 0.1913\n",
      "Epoch 33  Training Accuracy: 0.9628  Training Loss: 0.1208  Validation Accuracy: 0.9434  Validation Loss: 0.1931\n",
      "Epoch 34  Training Accuracy: 0.9633  Training Loss: 0.1175  Validation Accuracy: 0.9523  Validation Loss: 0.1625\n",
      "Epoch 35  Training Accuracy: 0.9644  Training Loss: 0.1144  Validation Accuracy: 0.9439  Validation Loss: 0.2062\n",
      "Epoch 36  Training Accuracy: 0.9637  Training Loss: 0.1169  Validation Accuracy: 0.9433  Validation Loss: 0.1950\n",
      "Epoch 37  Training Accuracy: 0.9651  Training Loss: 0.1126  Validation Accuracy: 0.9453  Validation Loss: 0.1878\n",
      "Epoch 38  Training Accuracy: 0.9663  Training Loss: 0.1087  Validation Accuracy: 0.9478  Validation Loss: 0.1813\n",
      "Epoch 39  Training Accuracy: 0.9656  Training Loss: 0.1101  Validation Accuracy: 0.9370  Validation Loss: 0.2300\n",
      "Epoch 40  Training Accuracy: 0.9670  Training Loss: 0.1062  Validation Accuracy: 0.9433  Validation Loss: 0.1942\n",
      "Epoch 41  Training Accuracy: 0.9672  Training Loss: 0.1046  Validation Accuracy: 0.9474  Validation Loss: 0.1833\n",
      "Epoch 42  Training Accuracy: 0.9679  Training Loss: 0.1046  Validation Accuracy: 0.9467  Validation Loss: 0.1822\n",
      "Epoch 43  Training Accuracy: 0.9685  Training Loss: 0.1022  Validation Accuracy: 0.9500  Validation Loss: 0.1777\n",
      "Epoch 44  Training Accuracy: 0.9691  Training Loss: 0.1003  Validation Accuracy: 0.9384  Validation Loss: 0.2257\n",
      "Epoch 45  Training Accuracy: 0.9689  Training Loss: 0.0998  Validation Accuracy: 0.9521  Validation Loss: 0.1744\n",
      "Epoch 46  Training Accuracy: 0.9687  Training Loss: 0.0997  Validation Accuracy: 0.9497  Validation Loss: 0.1764\n",
      "Epoch 47  Training Accuracy: 0.9692  Training Loss: 0.0984  Validation Accuracy: 0.9472  Validation Loss: 0.1834\n",
      "Epoch 48  Training Accuracy: 0.9701  Training Loss: 0.0936  Validation Accuracy: 0.9450  Validation Loss: 0.1968\n",
      "Epoch 49  Training Accuracy: 0.9697  Training Loss: 0.0953  Validation Accuracy: 0.9459  Validation Loss: 0.1916\n"
     ]
    }
   ],
   "source": [
    "### Create the network string represetation \n",
    "\n",
    "total_input_size  = 28*28\n",
    "total_ouput_size  = 10\n",
    "dict_network      = dict_network\n",
    "min_amount_layers = 1\n",
    "max_amount_layers = 50\n",
    "drop_layer_rate   = 0.5\n",
    "norm_layer_rate   = 0.5\n",
    "\n",
    "string_model_1 = dict_to_string_representation_network(total_input_size, \n",
    "                                                    total_ouput_size, \n",
    "                                                    dict_network, \n",
    "                                                    min_amount_layers, \n",
    "                                                    max_amount_layers,\n",
    "                                                    drop_layer_rate, \n",
    "                                                    norm_layer_rate)\n",
    "\n",
    "\n",
    "### transfer the network string represetation to a pytorch model \n",
    "\n",
    "model_1 = Net(string_model_1)\n",
    "print(model_1)\n",
    "\n",
    "\n",
    "### Create the optimizer string represetation \n",
    "\n",
    "opt_string_0 = dict_to_string_representation_optimizer(dict_optimizer)\n",
    "\n",
    "\n",
    "### transfer the string represetation to a pytorch optimizer \n",
    "\n",
    "optimizer_1 = parse_opt_string(opt_string_0, model_1)\n",
    "print(optimizer_1)\n",
    "\n",
    "\n",
    "### Train the model \n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_with_validation(model_1, \n",
    "                      train_loader, \n",
    "                      validation_loader, \n",
    "                      loss_fn, \n",
    "                      optimizer_1, \n",
    "                      num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74260f53",
   "metadata": {},
   "source": [
    "#### 6.1.2 Train Network 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "ac939353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "    (1): SELU()\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (3): LayerNorm((128,), eps=0.001, elementwise_affine=True)\n",
      "    (4): SiLU()\n",
      "    (5): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (6): Sigmoid()\n",
      "    (7): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (8): GELU()\n",
      "    (9): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
      ")\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.8, 0.9)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0  Training Accuracy: 0.8523  Training Loss: 0.5812  Validation Accuracy: 0.9302  Validation Loss: 0.2346\n",
      "Epoch 1  Training Accuracy: 0.9427  Training Loss: 0.1904  Validation Accuracy: 0.9523  Validation Loss: 0.1567\n",
      "Epoch 2  Training Accuracy: 0.9589  Training Loss: 0.1358  Validation Accuracy: 0.9632  Validation Loss: 0.1241\n",
      "Epoch 3  Training Accuracy: 0.9673  Training Loss: 0.1081  Validation Accuracy: 0.9666  Validation Loss: 0.1115\n",
      "Epoch 4  Training Accuracy: 0.9728  Training Loss: 0.0909  Validation Accuracy: 0.9710  Validation Loss: 0.0958\n",
      "Epoch 5  Training Accuracy: 0.9758  Training Loss: 0.0783  Validation Accuracy: 0.9721  Validation Loss: 0.0976\n",
      "Epoch 6  Training Accuracy: 0.9794  Training Loss: 0.0682  Validation Accuracy: 0.9740  Validation Loss: 0.0928\n",
      "Epoch 7  Training Accuracy: 0.9816  Training Loss: 0.0601  Validation Accuracy: 0.9728  Validation Loss: 0.0922\n",
      "Epoch 8  Training Accuracy: 0.9831  Training Loss: 0.0536  Validation Accuracy: 0.9768  Validation Loss: 0.0848\n",
      "Epoch 9  Training Accuracy: 0.9851  Training Loss: 0.0481  Validation Accuracy: 0.9759  Validation Loss: 0.0875\n",
      "Epoch 10  Training Accuracy: 0.9865  Training Loss: 0.0439  Validation Accuracy: 0.9716  Validation Loss: 0.1002\n",
      "Epoch 11  Training Accuracy: 0.9876  Training Loss: 0.0393  Validation Accuracy: 0.9741  Validation Loss: 0.0946\n",
      "Epoch 12  Training Accuracy: 0.9886  Training Loss: 0.0367  Validation Accuracy: 0.9723  Validation Loss: 0.1012\n",
      "Epoch 13  Training Accuracy: 0.9900  Training Loss: 0.0326  Validation Accuracy: 0.9764  Validation Loss: 0.0828\n",
      "Epoch 14  Training Accuracy: 0.9907  Training Loss: 0.0302  Validation Accuracy: 0.9748  Validation Loss: 0.0957\n",
      "Epoch 15  Training Accuracy: 0.9912  Training Loss: 0.0277  Validation Accuracy: 0.9742  Validation Loss: 0.0989\n",
      "Epoch 16  Training Accuracy: 0.9923  Training Loss: 0.0249  Validation Accuracy: 0.9737  Validation Loss: 0.1070\n",
      "Epoch 17  Training Accuracy: 0.9928  Training Loss: 0.0233  Validation Accuracy: 0.9766  Validation Loss: 0.0973\n",
      "Epoch 18  Training Accuracy: 0.9936  Training Loss: 0.0211  Validation Accuracy: 0.9767  Validation Loss: 0.0974\n",
      "Epoch 19  Training Accuracy: 0.9941  Training Loss: 0.0195  Validation Accuracy: 0.9751  Validation Loss: 0.0994\n",
      "Epoch 20  Training Accuracy: 0.9946  Training Loss: 0.0173  Validation Accuracy: 0.9761  Validation Loss: 0.1011\n",
      "Epoch 21  Training Accuracy: 0.9948  Training Loss: 0.0170  Validation Accuracy: 0.9759  Validation Loss: 0.1052\n",
      "Epoch 22  Training Accuracy: 0.9949  Training Loss: 0.0152  Validation Accuracy: 0.9760  Validation Loss: 0.1011\n",
      "Epoch 23  Training Accuracy: 0.9955  Training Loss: 0.0146  Validation Accuracy: 0.9759  Validation Loss: 0.1085\n",
      "Epoch 24  Training Accuracy: 0.9955  Training Loss: 0.0133  Validation Accuracy: 0.9733  Validation Loss: 0.1175\n",
      "Epoch 25  Training Accuracy: 0.9962  Training Loss: 0.0115  Validation Accuracy: 0.9756  Validation Loss: 0.1195\n",
      "Epoch 26  Training Accuracy: 0.9962  Training Loss: 0.0111  Validation Accuracy: 0.9759  Validation Loss: 0.1171\n",
      "Epoch 27  Training Accuracy: 0.9971  Training Loss: 0.0095  Validation Accuracy: 0.9721  Validation Loss: 0.1376\n",
      "Epoch 28  Training Accuracy: 0.9968  Training Loss: 0.0102  Validation Accuracy: 0.9746  Validation Loss: 0.1304\n",
      "Epoch 29  Training Accuracy: 0.9970  Training Loss: 0.0094  Validation Accuracy: 0.9752  Validation Loss: 0.1282\n",
      "Epoch 30  Training Accuracy: 0.9970  Training Loss: 0.0086  Validation Accuracy: 0.9754  Validation Loss: 0.1290\n",
      "Epoch 31  Training Accuracy: 0.9974  Training Loss: 0.0075  Validation Accuracy: 0.9756  Validation Loss: 0.1323\n",
      "Epoch 32  Training Accuracy: 0.9976  Training Loss: 0.0070  Validation Accuracy: 0.9731  Validation Loss: 0.1410\n",
      "Epoch 33  Training Accuracy: 0.9978  Training Loss: 0.0063  Validation Accuracy: 0.9741  Validation Loss: 0.1345\n",
      "Epoch 34  Training Accuracy: 0.9976  Training Loss: 0.0073  Validation Accuracy: 0.9746  Validation Loss: 0.1423\n",
      "Epoch 35  Training Accuracy: 0.9981  Training Loss: 0.0054  Validation Accuracy: 0.9737  Validation Loss: 0.1499\n",
      "Epoch 36  Training Accuracy: 0.9978  Training Loss: 0.0064  Validation Accuracy: 0.9760  Validation Loss: 0.1296\n",
      "Epoch 37  Training Accuracy: 0.9983  Training Loss: 0.0051  Validation Accuracy: 0.9746  Validation Loss: 0.1416\n",
      "Epoch 38  Training Accuracy: 0.9983  Training Loss: 0.0049  Validation Accuracy: 0.9754  Validation Loss: 0.1409\n",
      "Epoch 39  Training Accuracy: 0.9982  Training Loss: 0.0050  Validation Accuracy: 0.9737  Validation Loss: 0.1577\n",
      "Epoch 40  Training Accuracy: 0.9984  Training Loss: 0.0047  Validation Accuracy: 0.9737  Validation Loss: 0.1544\n",
      "Epoch 41  Training Accuracy: 0.9985  Training Loss: 0.0043  Validation Accuracy: 0.9773  Validation Loss: 0.1456\n",
      "Epoch 42  Training Accuracy: 0.9982  Training Loss: 0.0051  Validation Accuracy: 0.9741  Validation Loss: 0.1711\n",
      "Epoch 43  Training Accuracy: 0.9987  Training Loss: 0.0038  Validation Accuracy: 0.9748  Validation Loss: 0.1590\n",
      "Epoch 44  Training Accuracy: 0.9987  Training Loss: 0.0043  Validation Accuracy: 0.9732  Validation Loss: 0.1734\n",
      "Epoch 45  Training Accuracy: 0.9987  Training Loss: 0.0038  Validation Accuracy: 0.9739  Validation Loss: 0.1697\n",
      "Epoch 46  Training Accuracy: 0.9987  Training Loss: 0.0033  Validation Accuracy: 0.9757  Validation Loss: 0.1600\n",
      "Epoch 47  Training Accuracy: 0.9989  Training Loss: 0.0032  Validation Accuracy: 0.9767  Validation Loss: 0.1615\n",
      "Epoch 48  Training Accuracy: 0.9987  Training Loss: 0.0037  Validation Accuracy: 0.9756  Validation Loss: 0.1596\n",
      "Epoch 49  Training Accuracy: 0.9991  Training Loss: 0.0028  Validation Accuracy: 0.9751  Validation Loss: 0.1712\n"
     ]
    }
   ],
   "source": [
    "### Create the network string represetation \n",
    "\n",
    "total_input_size  = 28*28\n",
    "total_ouput_size  = 10\n",
    "dict_network      = dict_network\n",
    "min_amount_layers = 1\n",
    "max_amount_layers = 50\n",
    "drop_layer_rate   = 0.5\n",
    "norm_layer_rate   = 0.5\n",
    "\n",
    "string_model_2 = dict_to_string_representation_network(total_input_size, \n",
    "                                                    total_ouput_size, \n",
    "                                                    dict_network, \n",
    "                                                    min_amount_layers, \n",
    "                                                    max_amount_layers,\n",
    "                                                    drop_layer_rate, \n",
    "                                                    norm_layer_rate)\n",
    "\n",
    "\n",
    "### transfer the network string represetation to a pytorch model \n",
    "\n",
    "model_2 = Net(string_model_2)\n",
    "print(model_2)\n",
    "\n",
    "\n",
    "### transfer the string represetation to a pytorch optimizer \n",
    "\n",
    "optimizer_2 = parse_opt_string(opt_string_0, model_2)\n",
    "print(optimizer_2)\n",
    "\n",
    "\n",
    "### Train the model \n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_with_validation(model_2, \n",
    "                      train_loader, \n",
    "                      validation_loader, \n",
    "                      loss_fn, \n",
    "                      optimizer_2, \n",
    "                      num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98fd504",
   "metadata": {},
   "source": [
    "#### 6.1.3 Perform Crossover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "979220db",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossover_string_1, crossover_string_2 = crossover(string_model_1, string_model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c13e8d",
   "metadata": {},
   "source": [
    "#### 6.1.4 Train Network 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "aeff5cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): BatchNorm1d(512, eps=0.1, momentum=0.5, affine=True, track_running_stats=True)\n",
      "    (2): AlphaDropout(p=0.1, inplace=False)\n",
      "    (3): Sigmoid()\n",
      "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (5): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (6): Sigmoid()\n",
      "    (7): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (8): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (9): ELU(alpha=1.0)\n",
      "    (10): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (11): LayerNorm((128,), eps=0.0001, elementwise_affine=True)\n",
      "    (12): Dropout(p=0.1, inplace=False)\n",
      "    (13): ELU(alpha=1.0)\n",
      "    (14): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (15): SELU()\n",
      "    (16): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
      ")\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.8, 0.9)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0  Training Accuracy: 0.7865  Training Loss: 0.6538  Validation Accuracy: 0.9020  Validation Loss: 0.3270\n",
      "Epoch 1  Training Accuracy: 0.8792  Training Loss: 0.3903  Validation Accuracy: 0.9128  Validation Loss: 0.2857\n",
      "Epoch 2  Training Accuracy: 0.8972  Training Loss: 0.3330  Validation Accuracy: 0.9302  Validation Loss: 0.2326\n",
      "Epoch 3  Training Accuracy: 0.9086  Training Loss: 0.2958  Validation Accuracy: 0.9229  Validation Loss: 0.2481\n",
      "Epoch 4  Training Accuracy: 0.9145  Training Loss: 0.2744  Validation Accuracy: 0.9398  Validation Loss: 0.1994\n",
      "Epoch 5  Training Accuracy: 0.9207  Training Loss: 0.2548  Validation Accuracy: 0.9360  Validation Loss: 0.2025\n",
      "Epoch 6  Training Accuracy: 0.9271  Training Loss: 0.2348  Validation Accuracy: 0.9413  Validation Loss: 0.1877\n",
      "Epoch 7  Training Accuracy: 0.9309  Training Loss: 0.2203  Validation Accuracy: 0.9396  Validation Loss: 0.1937\n",
      "Epoch 8  Training Accuracy: 0.9343  Training Loss: 0.2099  Validation Accuracy: 0.9509  Validation Loss: 0.1628\n",
      "Epoch 9  Training Accuracy: 0.9369  Training Loss: 0.2035  Validation Accuracy: 0.9448  Validation Loss: 0.1813\n",
      "Epoch 10  Training Accuracy: 0.9396  Training Loss: 0.1935  Validation Accuracy: 0.9468  Validation Loss: 0.1815\n",
      "Epoch 11  Training Accuracy: 0.9424  Training Loss: 0.1861  Validation Accuracy: 0.9479  Validation Loss: 0.1652\n",
      "Epoch 12  Training Accuracy: 0.9446  Training Loss: 0.1789  Validation Accuracy: 0.9471  Validation Loss: 0.1786\n",
      "Epoch 13  Training Accuracy: 0.9455  Training Loss: 0.1748  Validation Accuracy: 0.9436  Validation Loss: 0.1886\n",
      "Epoch 14  Training Accuracy: 0.9479  Training Loss: 0.1675  Validation Accuracy: 0.9430  Validation Loss: 0.1942\n",
      "Epoch 15  Training Accuracy: 0.9492  Training Loss: 0.1624  Validation Accuracy: 0.9484  Validation Loss: 0.1732\n",
      "Epoch 16  Training Accuracy: 0.9512  Training Loss: 0.1598  Validation Accuracy: 0.9569  Validation Loss: 0.1479\n",
      "Epoch 17  Training Accuracy: 0.9526  Training Loss: 0.1531  Validation Accuracy: 0.9597  Validation Loss: 0.1326\n",
      "Epoch 18  Training Accuracy: 0.9539  Training Loss: 0.1494  Validation Accuracy: 0.9607  Validation Loss: 0.1329\n",
      "Epoch 19  Training Accuracy: 0.9549  Training Loss: 0.1435  Validation Accuracy: 0.9612  Validation Loss: 0.1329\n",
      "Epoch 20  Training Accuracy: 0.9559  Training Loss: 0.1401  Validation Accuracy: 0.9581  Validation Loss: 0.1430\n",
      "Epoch 21  Training Accuracy: 0.9580  Training Loss: 0.1356  Validation Accuracy: 0.9486  Validation Loss: 0.1779\n",
      "Epoch 22  Training Accuracy: 0.9585  Training Loss: 0.1307  Validation Accuracy: 0.9609  Validation Loss: 0.1362\n",
      "Epoch 23  Training Accuracy: 0.9602  Training Loss: 0.1271  Validation Accuracy: 0.9582  Validation Loss: 0.1470\n",
      "Epoch 24  Training Accuracy: 0.9616  Training Loss: 0.1238  Validation Accuracy: 0.9585  Validation Loss: 0.1495\n",
      "Epoch 25  Training Accuracy: 0.9608  Training Loss: 0.1231  Validation Accuracy: 0.9579  Validation Loss: 0.1412\n",
      "Epoch 26  Training Accuracy: 0.9630  Training Loss: 0.1181  Validation Accuracy: 0.9574  Validation Loss: 0.1504\n",
      "Epoch 27  Training Accuracy: 0.9641  Training Loss: 0.1120  Validation Accuracy: 0.9647  Validation Loss: 0.1179\n",
      "Epoch 28  Training Accuracy: 0.9655  Training Loss: 0.1086  Validation Accuracy: 0.9630  Validation Loss: 0.1358\n",
      "Epoch 29  Training Accuracy: 0.9658  Training Loss: 0.1061  Validation Accuracy: 0.9641  Validation Loss: 0.1222\n",
      "Epoch 30  Training Accuracy: 0.9664  Training Loss: 0.1074  Validation Accuracy: 0.9662  Validation Loss: 0.1167\n",
      "Epoch 31  Training Accuracy: 0.9683  Training Loss: 0.1017  Validation Accuracy: 0.9659  Validation Loss: 0.1198\n",
      "Epoch 32  Training Accuracy: 0.9680  Training Loss: 0.1011  Validation Accuracy: 0.9679  Validation Loss: 0.1220\n",
      "Epoch 33  Training Accuracy: 0.9691  Training Loss: 0.0973  Validation Accuracy: 0.9653  Validation Loss: 0.1207\n",
      "Epoch 34  Training Accuracy: 0.9694  Training Loss: 0.0977  Validation Accuracy: 0.9640  Validation Loss: 0.1195\n",
      "Epoch 35  Training Accuracy: 0.9699  Training Loss: 0.0941  Validation Accuracy: 0.9616  Validation Loss: 0.1360\n",
      "Epoch 36  Training Accuracy: 0.9701  Training Loss: 0.0941  Validation Accuracy: 0.9659  Validation Loss: 0.1197\n",
      "Epoch 37  Training Accuracy: 0.9718  Training Loss: 0.0907  Validation Accuracy: 0.9709  Validation Loss: 0.1040\n",
      "Epoch 38  Training Accuracy: 0.9722  Training Loss: 0.0884  Validation Accuracy: 0.9664  Validation Loss: 0.1183\n",
      "Epoch 39  Training Accuracy: 0.9732  Training Loss: 0.0845  Validation Accuracy: 0.9648  Validation Loss: 0.1249\n",
      "Epoch 40  Training Accuracy: 0.9732  Training Loss: 0.0836  Validation Accuracy: 0.9660  Validation Loss: 0.1261\n",
      "Epoch 41  Training Accuracy: 0.9735  Training Loss: 0.0827  Validation Accuracy: 0.9688  Validation Loss: 0.1102\n",
      "Epoch 42  Training Accuracy: 0.9746  Training Loss: 0.0815  Validation Accuracy: 0.9665  Validation Loss: 0.1220\n",
      "Epoch 43  Training Accuracy: 0.9756  Training Loss: 0.0768  Validation Accuracy: 0.9678  Validation Loss: 0.1130\n",
      "Epoch 44  Training Accuracy: 0.9743  Training Loss: 0.0800  Validation Accuracy: 0.9703  Validation Loss: 0.1051\n",
      "Epoch 45  Training Accuracy: 0.9760  Training Loss: 0.0761  Validation Accuracy: 0.9664  Validation Loss: 0.1192\n",
      "Epoch 46  Training Accuracy: 0.9768  Training Loss: 0.0734  Validation Accuracy: 0.9705  Validation Loss: 0.1089\n",
      "Epoch 47  Training Accuracy: 0.9775  Training Loss: 0.0727  Validation Accuracy: 0.9692  Validation Loss: 0.1119\n",
      "Epoch 48  Training Accuracy: 0.9771  Training Loss: 0.0717  Validation Accuracy: 0.9680  Validation Loss: 0.1150\n",
      "Epoch 49  Training Accuracy: 0.9766  Training Loss: 0.0710  Validation Accuracy: 0.9704  Validation Loss: 0.1143\n"
     ]
    }
   ],
   "source": [
    "### transfer the network string represetation to a pytorch model \n",
    "\n",
    "model_1_1 = Net(crossover_string_1)\n",
    "print(model_1_1)\n",
    "\n",
    "\n",
    "### transfer the string represetation to a pytorch optimizer \n",
    "\n",
    "optimizer_1_1 = parse_opt_string(opt_string_0, model_1_1)\n",
    "print(optimizer_1_1)\n",
    "\n",
    "\n",
    "### Train the model \n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_with_validation(model_1_1, \n",
    "                      train_loader, \n",
    "                      validation_loader, \n",
    "                      loss_fn, \n",
    "                      optimizer_1_1, \n",
    "                      num_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4002af",
   "metadata": {},
   "source": [
    "#### 6.1.5 Train Network 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "78c4caee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): BatchNorm1d(512, eps=0.1, momentum=0.5, affine=True, track_running_stats=True)\n",
      "    (2): AlphaDropout(p=0.1, inplace=False)\n",
      "    (3): Sigmoid()\n",
      "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (5): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (6): Sigmoid()\n",
      "    (7): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (8): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (9): ELU(alpha=1.0)\n",
      "    (10): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (11): LayerNorm((128,), eps=0.0001, elementwise_affine=True)\n",
      "    (12): Dropout(p=0.1, inplace=False)\n",
      "    (13): ELU(alpha=1.0)\n",
      "    (14): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (15): SELU()\n",
      "    (16): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
      ")\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.8, 0.9)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0  Training Accuracy: 0.7849  Training Loss: 0.6676  Validation Accuracy: 0.9113  Validation Loss: 0.3019\n",
      "Epoch 1  Training Accuracy: 0.8849  Training Loss: 0.3774  Validation Accuracy: 0.9103  Validation Loss: 0.2942\n",
      "Epoch 2  Training Accuracy: 0.8971  Training Loss: 0.3315  Validation Accuracy: 0.9201  Validation Loss: 0.2599\n",
      "Epoch 3  Training Accuracy: 0.9072  Training Loss: 0.2998  Validation Accuracy: 0.9310  Validation Loss: 0.2213\n",
      "Epoch 4  Training Accuracy: 0.9139  Training Loss: 0.2807  Validation Accuracy: 0.9318  Validation Loss: 0.2182\n",
      "Epoch 5  Training Accuracy: 0.9198  Training Loss: 0.2598  Validation Accuracy: 0.9344  Validation Loss: 0.2089\n",
      "Epoch 6  Training Accuracy: 0.9250  Training Loss: 0.2450  Validation Accuracy: 0.9407  Validation Loss: 0.1869\n",
      "Epoch 7  Training Accuracy: 0.9288  Training Loss: 0.2278  Validation Accuracy: 0.9365  Validation Loss: 0.2057\n",
      "Epoch 8  Training Accuracy: 0.9311  Training Loss: 0.2212  Validation Accuracy: 0.9485  Validation Loss: 0.1671\n",
      "Epoch 9  Training Accuracy: 0.9337  Training Loss: 0.2140  Validation Accuracy: 0.9486  Validation Loss: 0.1659\n",
      "Epoch 10  Training Accuracy: 0.9362  Training Loss: 0.2022  Validation Accuracy: 0.9434  Validation Loss: 0.1870\n",
      "Epoch 11  Training Accuracy: 0.9387  Training Loss: 0.1940  Validation Accuracy: 0.9363  Validation Loss: 0.2114\n",
      "Epoch 12  Training Accuracy: 0.9410  Training Loss: 0.1873  Validation Accuracy: 0.9514  Validation Loss: 0.1639\n",
      "Epoch 13  Training Accuracy: 0.9421  Training Loss: 0.1845  Validation Accuracy: 0.9482  Validation Loss: 0.1682\n",
      "Epoch 14  Training Accuracy: 0.9437  Training Loss: 0.1776  Validation Accuracy: 0.9567  Validation Loss: 0.1470\n",
      "Epoch 15  Training Accuracy: 0.9473  Training Loss: 0.1693  Validation Accuracy: 0.9516  Validation Loss: 0.1664\n",
      "Epoch 16  Training Accuracy: 0.9478  Training Loss: 0.1664  Validation Accuracy: 0.9505  Validation Loss: 0.1647\n",
      "Epoch 17  Training Accuracy: 0.9499  Training Loss: 0.1614  Validation Accuracy: 0.9553  Validation Loss: 0.1512\n",
      "Epoch 18  Training Accuracy: 0.9512  Training Loss: 0.1545  Validation Accuracy: 0.9558  Validation Loss: 0.1549\n",
      "Epoch 19  Training Accuracy: 0.9525  Training Loss: 0.1517  Validation Accuracy: 0.9602  Validation Loss: 0.1424\n",
      "Epoch 20  Training Accuracy: 0.9542  Training Loss: 0.1473  Validation Accuracy: 0.9476  Validation Loss: 0.1798\n",
      "Epoch 21  Training Accuracy: 0.9553  Training Loss: 0.1430  Validation Accuracy: 0.9563  Validation Loss: 0.1475\n",
      "Epoch 22  Training Accuracy: 0.9567  Training Loss: 0.1354  Validation Accuracy: 0.9555  Validation Loss: 0.1504\n",
      "Epoch 23  Training Accuracy: 0.9578  Training Loss: 0.1338  Validation Accuracy: 0.9639  Validation Loss: 0.1252\n",
      "Epoch 24  Training Accuracy: 0.9598  Training Loss: 0.1279  Validation Accuracy: 0.9559  Validation Loss: 0.1528\n",
      "Epoch 25  Training Accuracy: 0.9609  Training Loss: 0.1244  Validation Accuracy: 0.9537  Validation Loss: 0.1571\n",
      "Epoch 26  Training Accuracy: 0.9621  Training Loss: 0.1205  Validation Accuracy: 0.9604  Validation Loss: 0.1383\n",
      "Epoch 27  Training Accuracy: 0.9634  Training Loss: 0.1167  Validation Accuracy: 0.9624  Validation Loss: 0.1322\n",
      "Epoch 28  Training Accuracy: 0.9647  Training Loss: 0.1124  Validation Accuracy: 0.9593  Validation Loss: 0.1361\n",
      "Epoch 29  Training Accuracy: 0.9655  Training Loss: 0.1100  Validation Accuracy: 0.9587  Validation Loss: 0.1387\n",
      "Epoch 30  Training Accuracy: 0.9654  Training Loss: 0.1102  Validation Accuracy: 0.9663  Validation Loss: 0.1160\n",
      "Epoch 31  Training Accuracy: 0.9674  Training Loss: 0.1043  Validation Accuracy: 0.9640  Validation Loss: 0.1195\n",
      "Epoch 32  Training Accuracy: 0.9682  Training Loss: 0.1016  Validation Accuracy: 0.9678  Validation Loss: 0.1144\n",
      "Epoch 33  Training Accuracy: 0.9682  Training Loss: 0.1008  Validation Accuracy: 0.9675  Validation Loss: 0.1091\n",
      "Epoch 34  Training Accuracy: 0.9694  Training Loss: 0.0965  Validation Accuracy: 0.9702  Validation Loss: 0.1013\n",
      "Epoch 35  Training Accuracy: 0.9713  Training Loss: 0.0913  Validation Accuracy: 0.9652  Validation Loss: 0.1296\n",
      "Epoch 36  Training Accuracy: 0.9707  Training Loss: 0.0917  Validation Accuracy: 0.9692  Validation Loss: 0.1079\n",
      "Epoch 37  Training Accuracy: 0.9715  Training Loss: 0.0895  Validation Accuracy: 0.9704  Validation Loss: 0.1098\n",
      "Epoch 38  Training Accuracy: 0.9729  Training Loss: 0.0863  Validation Accuracy: 0.9644  Validation Loss: 0.1295\n",
      "Epoch 39  Training Accuracy: 0.9732  Training Loss: 0.0835  Validation Accuracy: 0.9706  Validation Loss: 0.1004\n",
      "Epoch 40  Training Accuracy: 0.9735  Training Loss: 0.0831  Validation Accuracy: 0.9685  Validation Loss: 0.1146\n",
      "Epoch 41  Training Accuracy: 0.9743  Training Loss: 0.0821  Validation Accuracy: 0.9671  Validation Loss: 0.1183\n",
      "Epoch 42  Training Accuracy: 0.9753  Training Loss: 0.0778  Validation Accuracy: 0.9684  Validation Loss: 0.1223\n",
      "Epoch 43  Training Accuracy: 0.9759  Training Loss: 0.0764  Validation Accuracy: 0.9697  Validation Loss: 0.1111\n",
      "Epoch 44  Training Accuracy: 0.9755  Training Loss: 0.0767  Validation Accuracy: 0.9666  Validation Loss: 0.1241\n",
      "Epoch 45  Training Accuracy: 0.9764  Training Loss: 0.0748  Validation Accuracy: 0.9724  Validation Loss: 0.1015\n",
      "Epoch 46  Training Accuracy: 0.9764  Training Loss: 0.0738  Validation Accuracy: 0.9701  Validation Loss: 0.1040\n",
      "Epoch 47  Training Accuracy: 0.9776  Training Loss: 0.0711  Validation Accuracy: 0.9673  Validation Loss: 0.1233\n",
      "Epoch 48  Training Accuracy: 0.9784  Training Loss: 0.0696  Validation Accuracy: 0.9729  Validation Loss: 0.1034\n",
      "Epoch 49  Training Accuracy: 0.9776  Training Loss: 0.0684  Validation Accuracy: 0.9694  Validation Loss: 0.1137\n"
     ]
    }
   ],
   "source": [
    "### transfer the network string represetation to a pytorch model \n",
    "\n",
    "model_2_2 = Net(crossover_string_1)\n",
    "print(model_2_2)\n",
    "\n",
    "\n",
    "### transfer the string represetation to a pytorch optimizer \n",
    "\n",
    "optimizer_2_2 = parse_opt_string(opt_string_0, model_2_2)\n",
    "print(optimizer_2_2)\n",
    "\n",
    "\n",
    "### Train the model \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_with_validation(model_2_2, \n",
    "                      train_loader, \n",
    "                      validation_loader, \n",
    "                      loss_fn, \n",
    "                      optimizer_2_2, \n",
    "                      num_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b5ae0e",
   "metadata": {},
   "source": [
    "### 6.2. Experiment 2 - Network 3 //  Add layer mutation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87209d85",
   "metadata": {},
   "source": [
    "#### 6.2.1 Train Network 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "0d22f971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Training Accuracy: 0.1006  Training Loss: 2.3522  Validation Accuracy: 0.0958  Validation Loss: 2.3146\n",
      "Epoch 1  Training Accuracy: 0.1039  Training Loss: 2.3429  Validation Accuracy: 0.0974  Validation Loss: 2.3125\n",
      "Epoch 2  Training Accuracy: 0.1028  Training Loss: 2.3385  Validation Accuracy: 0.0932  Validation Loss: 2.3091\n",
      "Epoch 3  Training Accuracy: 0.0988  Training Loss: 2.3385  Validation Accuracy: 0.0980  Validation Loss: 2.3091\n",
      "Epoch 4  Training Accuracy: 0.1020  Training Loss: 2.3335  Validation Accuracy: 0.1007  Validation Loss: 2.3068\n",
      "Epoch 5  Training Accuracy: 0.1017  Training Loss: 2.3322  Validation Accuracy: 0.0980  Validation Loss: 2.3068\n",
      "Epoch 6  Training Accuracy: 0.1025  Training Loss: 2.3300  Validation Accuracy: 0.0958  Validation Loss: 2.3062\n",
      "Epoch 7  Training Accuracy: 0.1004  Training Loss: 2.3272  Validation Accuracy: 0.1067  Validation Loss: 2.3026\n",
      "Epoch 8  Training Accuracy: 0.1012  Training Loss: 2.3263  Validation Accuracy: 0.1037  Validation Loss: 2.3050\n",
      "Epoch 9  Training Accuracy: 0.1034  Training Loss: 2.3233  Validation Accuracy: 0.1111  Validation Loss: 2.3048\n",
      "Epoch 10  Training Accuracy: 0.1021  Training Loss: 2.3238  Validation Accuracy: 0.0997  Validation Loss: 2.3031\n",
      "Epoch 11  Training Accuracy: 0.1021  Training Loss: 2.3225  Validation Accuracy: 0.0855  Validation Loss: 2.3029\n",
      "Epoch 12  Training Accuracy: 0.1022  Training Loss: 2.3211  Validation Accuracy: 0.0965  Validation Loss: 2.3032\n",
      "Epoch 13  Training Accuracy: 0.1031  Training Loss: 2.3205  Validation Accuracy: 0.0974  Validation Loss: 2.3046\n",
      "Epoch 14  Training Accuracy: 0.1015  Training Loss: 2.3221  Validation Accuracy: 0.1041  Validation Loss: 2.3043\n",
      "Epoch 15  Training Accuracy: 0.1008  Training Loss: 2.3194  Validation Accuracy: 0.0958  Validation Loss: 2.3022\n",
      "Epoch 16  Training Accuracy: 0.1038  Training Loss: 2.3180  Validation Accuracy: 0.0581  Validation Loss: 2.3031\n",
      "Epoch 17  Training Accuracy: 0.1016  Training Loss: 2.3180  Validation Accuracy: 0.1406  Validation Loss: 2.3030\n",
      "Epoch 18  Training Accuracy: 0.1001  Training Loss: 2.3175  Validation Accuracy: 0.0974  Validation Loss: 2.3032\n",
      "Epoch 19  Training Accuracy: 0.1011  Training Loss: 2.3172  Validation Accuracy: 0.1024  Validation Loss: 2.3015\n",
      "Epoch 20  Training Accuracy: 0.1021  Training Loss: 2.3159  Validation Accuracy: 0.1415  Validation Loss: 2.3029\n",
      "Epoch 21  Training Accuracy: 0.1046  Training Loss: 2.3135  Validation Accuracy: 0.0958  Validation Loss: 2.3035\n",
      "Epoch 22  Training Accuracy: 0.1048  Training Loss: 2.3156  Validation Accuracy: 0.0958  Validation Loss: 2.3024\n",
      "Epoch 23  Training Accuracy: 0.1013  Training Loss: 2.3153  Validation Accuracy: 0.1135  Validation Loss: 2.3007\n",
      "Epoch 24  Training Accuracy: 0.1029  Training Loss: 2.3147  Validation Accuracy: 0.0958  Validation Loss: 2.3028\n",
      "Epoch 25  Training Accuracy: 0.1033  Training Loss: 2.3147  Validation Accuracy: 0.0958  Validation Loss: 2.3044\n",
      "Epoch 26  Training Accuracy: 0.1021  Training Loss: 2.3148  Validation Accuracy: 0.0958  Validation Loss: 2.3036\n",
      "Epoch 27  Training Accuracy: 0.1015  Training Loss: 2.3151  Validation Accuracy: 0.0958  Validation Loss: 2.3034\n",
      "Epoch 28  Training Accuracy: 0.1042  Training Loss: 2.3131  Validation Accuracy: 0.0955  Validation Loss: 2.3025\n",
      "Epoch 29  Training Accuracy: 0.1067  Training Loss: 2.3128  Validation Accuracy: 0.0958  Validation Loss: 2.3039\n",
      "Epoch 30  Training Accuracy: 0.1009  Training Loss: 2.3140  Validation Accuracy: 0.0954  Validation Loss: 2.3013\n",
      "Epoch 31  Training Accuracy: 0.1019  Training Loss: 2.3121  Validation Accuracy: 0.0958  Validation Loss: 2.3032\n",
      "Epoch 32  Training Accuracy: 0.1034  Training Loss: 2.3133  Validation Accuracy: 0.0958  Validation Loss: 2.3024\n",
      "Epoch 33  Training Accuracy: 0.1054  Training Loss: 2.3122  Validation Accuracy: 0.0504  Validation Loss: 2.3030\n",
      "Epoch 34  Training Accuracy: 0.1055  Training Loss: 2.3125  Validation Accuracy: 0.0958  Validation Loss: 2.3036\n",
      "Epoch 35  Training Accuracy: 0.1027  Training Loss: 2.3122  Validation Accuracy: 0.0639  Validation Loss: 2.2998\n",
      "Epoch 36  Training Accuracy: 0.1028  Training Loss: 2.3130  Validation Accuracy: 0.0958  Validation Loss: 2.3052\n",
      "Epoch 37  Training Accuracy: 0.1035  Training Loss: 2.3121  Validation Accuracy: 0.1156  Validation Loss: 2.3009\n",
      "Epoch 38  Training Accuracy: 0.1021  Training Loss: 2.3119  Validation Accuracy: 0.0958  Validation Loss: 2.3014\n",
      "Epoch 39  Training Accuracy: 0.1051  Training Loss: 2.3119  Validation Accuracy: 0.0958  Validation Loss: 2.3016\n",
      "Epoch 40  Training Accuracy: 0.1032  Training Loss: 2.3118  Validation Accuracy: 0.1164  Validation Loss: 2.3010\n",
      "Epoch 41  Training Accuracy: 0.1037  Training Loss: 2.3119  Validation Accuracy: 0.0980  Validation Loss: 2.3001\n",
      "Epoch 42  Training Accuracy: 0.1025  Training Loss: 2.3106  Validation Accuracy: 0.0959  Validation Loss: 2.3025\n",
      "Epoch 43  Training Accuracy: 0.1033  Training Loss: 2.3113  Validation Accuracy: 0.1069  Validation Loss: 2.3014\n",
      "Epoch 44  Training Accuracy: 0.1016  Training Loss: 2.3115  Validation Accuracy: 0.0899  Validation Loss: 2.3008\n",
      "Epoch 45  Training Accuracy: 0.1022  Training Loss: 2.3116  Validation Accuracy: 0.0923  Validation Loss: 2.3004\n",
      "Epoch 46  Training Accuracy: 0.1029  Training Loss: 2.3115  Validation Accuracy: 0.1135  Validation Loss: 2.2993\n",
      "Epoch 47  Training Accuracy: 0.1032  Training Loss: 2.3110  Validation Accuracy: 0.0958  Validation Loss: 2.3019\n",
      "Epoch 48  Training Accuracy: 0.1060  Training Loss: 2.3106  Validation Accuracy: 0.0958  Validation Loss: 2.3006\n",
      "Epoch 49  Training Accuracy: 0.1046  Training Loss: 2.3108  Validation Accuracy: 0.0958  Validation Loss: 2.3022\n"
     ]
    }
   ],
   "source": [
    "### Create the network string represetation \n",
    "\n",
    "\n",
    "total_input_size  = 28*28\n",
    "total_ouput_size  = 10\n",
    "dict_network      = dict_network\n",
    "min_amount_layers = 1\n",
    "max_amount_layers = 50\n",
    "drop_layer_rate   = 0.5\n",
    "norm_layer_rate   = 0.5\n",
    "\n",
    "string_model_3 = dict_to_string_representation_network(total_input_size, \n",
    "                                                    total_ouput_size, \n",
    "                                                    dict_network, \n",
    "                                                    min_amount_layers, \n",
    "                                                    max_amount_layers,\n",
    "                                                    drop_layer_rate, \n",
    "                                                    norm_layer_rate)\n",
    "\n",
    "\n",
    "### transfer the network string represetation to a pytorch model \n",
    "\n",
    "model_3 = Net(string_model_3)\n",
    "#print(model_3)\n",
    "\n",
    "\n",
    "### Create the optimizer string represetation \n",
    "\n",
    "opt_string_3 = dict_to_string_representation_optimizer(dict_optimizer)\n",
    "\n",
    "\n",
    "### transfer the string represetation to a pytorch optimizer \n",
    "\n",
    "optimizer_3 = parse_opt_string(opt_string_3, model_3)\n",
    "#print(optimizer_3)\n",
    "\n",
    "\n",
    "### Train the model \n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_with_validation(model_3, \n",
    "                     train_loader, \n",
    "                      validation_loader, \n",
    "                      loss_fn, \n",
    "                      optimizer_3, \n",
    "                      num_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fe06fc",
   "metadata": {},
   "source": [
    "#### 6.2.2 Perform - Add layer mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "46dcd67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutation Length 19\n",
      "No Action needed\n",
      "11\n",
      "Integrity Check\n",
      "Act|CELU\n",
      "AlphaDropout|0.1\n",
      "Linear|128,512,True\n"
     ]
    }
   ],
   "source": [
    "mutated_layer_string_network = add_layer_mutation(string_model_3, dict_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "4008b951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): PReLU(num_parameters=1)\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): LayerNorm((256,), eps=0.001, elementwise_affine=True)\n",
      "    (4): Sigmoid()\n",
      "    (5): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (6): Dropout(p=0.3, inplace=False)\n",
      "    (7): Sigmoid()\n",
      "    (8): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (9): LayerNorm((128,), eps=0.0001, elementwise_affine=True)\n",
      "    (10): CELU(alpha=1.0)\n",
      "    (11): AlphaDropout(p=0.1, inplace=False)\n",
      "    (12): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (13): AlphaDropout(p=0.7, inplace=False)\n",
      "    (14): PReLU(num_parameters=1)\n",
      "    (15): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (16): Dropout(p=0.5, inplace=False)\n",
      "    (17): Sigmoid()\n",
      "    (18): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (19): BatchNorm1d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (20): AlphaDropout(p=0.3, inplace=False)\n",
      "    (21): SELU()\n",
      "    (22): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (23): LayerNorm((512,), eps=0.0001, elementwise_affine=True)\n",
      "    (24): AlphaDropout(p=0.1, inplace=False)\n",
      "    (25): CELU(alpha=1.0)\n",
      "    (26): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (27): BatchNorm1d(256, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (28): CELU(alpha=1.0)\n",
      "    (29): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (30): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (31): Dropout(p=0.7, inplace=False)\n",
      "    (32): ELU(alpha=1.0)\n",
      "    (33): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (34): BatchNorm1d(512, eps=0.001, momentum=0.9, affine=True, track_running_stats=True)\n",
      "    (35): GELU()\n",
      "    (36): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
      ")\n",
      "Adadelta (\n",
      "Parameter Group 0\n",
      "    eps: 1e-06\n",
      "    lr: 0.001\n",
      "    rho: 0.99\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0  Training Accuracy: 0.1032  Training Loss: 2.3513  Validation Accuracy: 0.1506  Validation Loss: 2.3128\n",
      "Epoch 1  Training Accuracy: 0.1012  Training Loss: 2.3428  Validation Accuracy: 0.1028  Validation Loss: 2.3171\n",
      "Epoch 2  Training Accuracy: 0.1002  Training Loss: 2.3398  Validation Accuracy: 0.1028  Validation Loss: 2.3098\n",
      "Epoch 3  Training Accuracy: 0.1021  Training Loss: 2.3379  Validation Accuracy: 0.1028  Validation Loss: 2.3107\n",
      "Epoch 4  Training Accuracy: 0.1040  Training Loss: 2.3340  Validation Accuracy: 0.1028  Validation Loss: 2.3072\n",
      "Epoch 5  Training Accuracy: 0.1020  Training Loss: 2.3312  Validation Accuracy: 0.1028  Validation Loss: 2.3063\n",
      "Epoch 6  Training Accuracy: 0.1029  Training Loss: 2.3285  Validation Accuracy: 0.1028  Validation Loss: 2.3098\n",
      "Epoch 7  Training Accuracy: 0.1031  Training Loss: 2.3271  Validation Accuracy: 0.1028  Validation Loss: 2.3054\n",
      "Epoch 8  Training Accuracy: 0.1027  Training Loss: 2.3267  Validation Accuracy: 0.1028  Validation Loss: 2.3041\n",
      "Epoch 9  Training Accuracy: 0.1022  Training Loss: 2.3241  Validation Accuracy: 0.1028  Validation Loss: 2.3066\n",
      "Epoch 10  Training Accuracy: 0.1021  Training Loss: 2.3231  Validation Accuracy: 0.1028  Validation Loss: 2.3080\n",
      "Epoch 11  Training Accuracy: 0.1005  Training Loss: 2.3223  Validation Accuracy: 0.1028  Validation Loss: 2.3038\n",
      "Epoch 12  Training Accuracy: 0.1048  Training Loss: 2.3203  Validation Accuracy: 0.1028  Validation Loss: 2.3038\n",
      "Epoch 13  Training Accuracy: 0.1026  Training Loss: 2.3216  Validation Accuracy: 0.1028  Validation Loss: 2.3042\n",
      "Epoch 14  Training Accuracy: 0.1003  Training Loss: 2.3190  Validation Accuracy: 0.1028  Validation Loss: 2.3052\n",
      "Epoch 15  Training Accuracy: 0.1018  Training Loss: 2.3202  Validation Accuracy: 0.1028  Validation Loss: 2.3034\n",
      "Epoch 16  Training Accuracy: 0.1029  Training Loss: 2.3169  Validation Accuracy: 0.1028  Validation Loss: 2.3043\n",
      "Epoch 17  Training Accuracy: 0.1036  Training Loss: 2.3187  Validation Accuracy: 0.1028  Validation Loss: 2.3055\n",
      "Epoch 18  Training Accuracy: 0.1017  Training Loss: 2.3168  Validation Accuracy: 0.1135  Validation Loss: 2.3038\n",
      "Epoch 19  Training Accuracy: 0.1032  Training Loss: 2.3186  Validation Accuracy: 0.1135  Validation Loss: 2.3029\n",
      "Epoch 20  Training Accuracy: 0.1032  Training Loss: 2.3171  Validation Accuracy: 0.1028  Validation Loss: 2.3025\n",
      "Epoch 21  Training Accuracy: 0.1035  Training Loss: 2.3157  Validation Accuracy: 0.1533  Validation Loss: 2.3013\n",
      "Epoch 22  Training Accuracy: 0.1019  Training Loss: 2.3163  Validation Accuracy: 0.1028  Validation Loss: 2.3019\n",
      "Epoch 23  Training Accuracy: 0.1030  Training Loss: 2.3160  Validation Accuracy: 0.1028  Validation Loss: 2.3022\n",
      "Epoch 24  Training Accuracy: 0.1053  Training Loss: 2.3139  Validation Accuracy: 0.1028  Validation Loss: 2.3039\n",
      "Epoch 25  Training Accuracy: 0.1025  Training Loss: 2.3148  Validation Accuracy: 0.1028  Validation Loss: 2.3023\n",
      "Epoch 26  Training Accuracy: 0.1054  Training Loss: 2.3140  Validation Accuracy: 0.1055  Validation Loss: 2.3021\n",
      "Epoch 27  Training Accuracy: 0.1023  Training Loss: 2.3145  Validation Accuracy: 0.1028  Validation Loss: 2.3026\n",
      "Epoch 28  Training Accuracy: 0.1028  Training Loss: 2.3136  Validation Accuracy: 0.1028  Validation Loss: 2.3042\n",
      "Epoch 29  Training Accuracy: 0.1048  Training Loss: 2.3131  Validation Accuracy: 0.0365  Validation Loss: 2.3022\n",
      "Epoch 30  Training Accuracy: 0.1036  Training Loss: 2.3126  Validation Accuracy: 0.1135  Validation Loss: 2.3022\n",
      "Epoch 31  Training Accuracy: 0.1041  Training Loss: 2.3118  Validation Accuracy: 0.1135  Validation Loss: 2.3008\n",
      "Epoch 32  Training Accuracy: 0.1046  Training Loss: 2.3120  Validation Accuracy: 0.1247  Validation Loss: 2.3021\n",
      "Epoch 33  Training Accuracy: 0.1040  Training Loss: 2.3127  Validation Accuracy: 0.1353  Validation Loss: 2.3031\n",
      "Epoch 34  Training Accuracy: 0.1020  Training Loss: 2.3124  Validation Accuracy: 0.1041  Validation Loss: 2.3031\n",
      "Epoch 35  Training Accuracy: 0.1038  Training Loss: 2.3119  Validation Accuracy: 0.1135  Validation Loss: 2.3022\n",
      "Epoch 36  Training Accuracy: 0.1063  Training Loss: 2.3126  Validation Accuracy: 0.1028  Validation Loss: 2.3022\n",
      "Epoch 37  Training Accuracy: 0.1059  Training Loss: 2.3107  Validation Accuracy: 0.1028  Validation Loss: 2.3059\n",
      "Epoch 38  Training Accuracy: 0.1031  Training Loss: 2.3125  Validation Accuracy: 0.1028  Validation Loss: 2.3025\n",
      "Epoch 39  Training Accuracy: 0.1035  Training Loss: 2.3116  Validation Accuracy: 0.1332  Validation Loss: 2.3021\n",
      "Epoch 40  Training Accuracy: 0.1041  Training Loss: 2.3110  Validation Accuracy: 0.1028  Validation Loss: 2.3011\n",
      "Epoch 41  Training Accuracy: 0.1031  Training Loss: 2.3106  Validation Accuracy: 0.1028  Validation Loss: 2.3022\n",
      "Epoch 42  Training Accuracy: 0.1032  Training Loss: 2.3115  Validation Accuracy: 0.1028  Validation Loss: 2.3027\n",
      "Epoch 43  Training Accuracy: 0.1036  Training Loss: 2.3105  Validation Accuracy: 0.1028  Validation Loss: 2.3036\n",
      "Epoch 44  Training Accuracy: 0.1012  Training Loss: 2.3113  Validation Accuracy: 0.1028  Validation Loss: 2.3027\n",
      "Epoch 45  Training Accuracy: 0.1062  Training Loss: 2.3112  Validation Accuracy: 0.1028  Validation Loss: 2.3026\n",
      "Epoch 46  Training Accuracy: 0.1045  Training Loss: 2.3109  Validation Accuracy: 0.1028  Validation Loss: 2.3030\n",
      "Epoch 47  Training Accuracy: 0.1026  Training Loss: 2.3111  Validation Accuracy: 0.1028  Validation Loss: 2.3009\n",
      "Epoch 48  Training Accuracy: 0.1004  Training Loss: 2.3116  Validation Accuracy: 0.1028  Validation Loss: 2.3026\n",
      "Epoch 49  Training Accuracy: 0.1024  Training Loss: 2.3105  Validation Accuracy: 0.1141  Validation Loss: 2.3003\n"
     ]
    }
   ],
   "source": [
    "### transfer the network string represetation to a pytorch model \n",
    "\n",
    "model_3_2 = Net(mutated_layer_string_network)\n",
    "print(model_3_2)\n",
    "\n",
    "\n",
    "### transfer the string represetation to a pytorch optimizer \n",
    "\n",
    "optimizer_3_2 = parse_opt_string(opt_string_3, model_3_2)\n",
    "print(optimizer_3_2)\n",
    "\n",
    "\n",
    "### Train the model \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_with_validation(model_3_2, \n",
    "                      train_loader, \n",
    "                      validation_loader, \n",
    "                      loss_fn, \n",
    "                      optimizer_3_2, \n",
    "                      num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d3a3e4",
   "metadata": {},
   "source": [
    "### 6.3 Experiment 3 - Network 4 //  Remove layer mutation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8743a2",
   "metadata": {},
   "source": [
    "#### 6.3.1 Train Network 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "6dc252a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): AlphaDropout(p=0.5, inplace=False)\n",
      "    (3): CELU(alpha=1.0)\n",
      "    (4): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (5): ELU(alpha=1.0)\n",
      "    (6): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (7): BatchNorm1d(128, eps=0.1, momentum=0.5, affine=True, track_running_stats=True)\n",
      "    (8): PReLU(num_parameters=1)\n",
      "    (9): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (10): AlphaDropout(p=0.5, inplace=False)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (13): BatchNorm1d(512, eps=0.1, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): SiLU()\n",
      "    (15): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (16): AlphaDropout(p=0.1, inplace=False)\n",
      "    (17): CELU(alpha=1.0)\n",
      "    (18): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (19): BatchNorm1d(128, eps=0.01, momentum=0.5, affine=True, track_running_stats=True)\n",
      "    (20): AlphaDropout(p=0.3, inplace=False)\n",
      "    (21): CELU(alpha=1.0)\n",
      "    (22): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
      ")\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.8, 0.9)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0  Training Accuracy: 0.6849  Training Loss: 0.9931  Validation Accuracy: 0.8759  Validation Loss: 0.5391\n",
      "Epoch 1  Training Accuracy: 0.8213  Training Loss: 0.5830  Validation Accuracy: 0.8766  Validation Loss: 0.5111\n",
      "Epoch 2  Training Accuracy: 0.8481  Training Loss: 0.5069  Validation Accuracy: 0.8672  Validation Loss: 0.4978\n",
      "Epoch 3  Training Accuracy: 0.8616  Training Loss: 0.4562  Validation Accuracy: 0.8597  Validation Loss: 0.4881\n",
      "Epoch 4  Training Accuracy: 0.8740  Training Loss: 0.4209  Validation Accuracy: 0.8288  Validation Loss: 0.5759\n",
      "Epoch 5  Training Accuracy: 0.8809  Training Loss: 0.3956  Validation Accuracy: 0.8222  Validation Loss: 0.5655\n",
      "Epoch 6  Training Accuracy: 0.8873  Training Loss: 0.3765  Validation Accuracy: 0.7932  Validation Loss: 0.6859\n",
      "Epoch 7  Training Accuracy: 0.8949  Training Loss: 0.3511  Validation Accuracy: 0.8258  Validation Loss: 0.5841\n",
      "Epoch 8  Training Accuracy: 0.8949  Training Loss: 0.3489  Validation Accuracy: 0.8182  Validation Loss: 0.5729\n",
      "Epoch 9  Training Accuracy: 0.9031  Training Loss: 0.3240  Validation Accuracy: 0.8217  Validation Loss: 0.5642\n",
      "Epoch 10  Training Accuracy: 0.9060  Training Loss: 0.3175  Validation Accuracy: 0.8143  Validation Loss: 0.5999\n",
      "Epoch 11  Training Accuracy: 0.9070  Training Loss: 0.3122  Validation Accuracy: 0.8230  Validation Loss: 0.5250\n",
      "Epoch 12  Training Accuracy: 0.9117  Training Loss: 0.2945  Validation Accuracy: 0.8238  Validation Loss: 0.5497\n",
      "Epoch 13  Training Accuracy: 0.9144  Training Loss: 0.2882  Validation Accuracy: 0.8394  Validation Loss: 0.4695\n",
      "Epoch 14  Training Accuracy: 0.9169  Training Loss: 0.2776  Validation Accuracy: 0.8176  Validation Loss: 0.5466\n",
      "Epoch 15  Training Accuracy: 0.9197  Training Loss: 0.2685  Validation Accuracy: 0.8385  Validation Loss: 0.4818\n",
      "Epoch 16  Training Accuracy: 0.9209  Training Loss: 0.2654  Validation Accuracy: 0.8455  Validation Loss: 0.4681\n",
      "Epoch 17  Training Accuracy: 0.9246  Training Loss: 0.2547  Validation Accuracy: 0.8524  Validation Loss: 0.4250\n",
      "Epoch 18  Training Accuracy: 0.9248  Training Loss: 0.2534  Validation Accuracy: 0.8632  Validation Loss: 0.4032\n",
      "Epoch 19  Training Accuracy: 0.9270  Training Loss: 0.2460  Validation Accuracy: 0.8849  Validation Loss: 0.3596\n",
      "Epoch 20  Training Accuracy: 0.9289  Training Loss: 0.2414  Validation Accuracy: 0.9071  Validation Loss: 0.2836\n",
      "Epoch 21  Training Accuracy: 0.9315  Training Loss: 0.2315  Validation Accuracy: 0.8923  Validation Loss: 0.3498\n",
      "Epoch 22  Training Accuracy: 0.9319  Training Loss: 0.2300  Validation Accuracy: 0.8879  Validation Loss: 0.3639\n",
      "Epoch 23  Training Accuracy: 0.9328  Training Loss: 0.2267  Validation Accuracy: 0.8728  Validation Loss: 0.4131\n",
      "Epoch 24  Training Accuracy: 0.9339  Training Loss: 0.2206  Validation Accuracy: 0.8890  Validation Loss: 0.3768\n",
      "Epoch 25  Training Accuracy: 0.9367  Training Loss: 0.2133  Validation Accuracy: 0.9013  Validation Loss: 0.3457\n",
      "Epoch 26  Training Accuracy: 0.9358  Training Loss: 0.2162  Validation Accuracy: 0.8968  Validation Loss: 0.3610\n",
      "Epoch 27  Training Accuracy: 0.9374  Training Loss: 0.2089  Validation Accuracy: 0.9128  Validation Loss: 0.3232\n",
      "Epoch 28  Training Accuracy: 0.9384  Training Loss: 0.2052  Validation Accuracy: 0.9097  Validation Loss: 0.3431\n",
      "Epoch 29  Training Accuracy: 0.9394  Training Loss: 0.2035  Validation Accuracy: 0.9129  Validation Loss: 0.3436\n",
      "Epoch 30  Training Accuracy: 0.9403  Training Loss: 0.1994  Validation Accuracy: 0.9198  Validation Loss: 0.2890\n",
      "Epoch 31  Training Accuracy: 0.9419  Training Loss: 0.1939  Validation Accuracy: 0.9151  Validation Loss: 0.3395\n",
      "Epoch 32  Training Accuracy: 0.9422  Training Loss: 0.1946  Validation Accuracy: 0.9240  Validation Loss: 0.3102\n",
      "Epoch 33  Training Accuracy: 0.9442  Training Loss: 0.1891  Validation Accuracy: 0.9278  Validation Loss: 0.3333\n",
      "Epoch 34  Training Accuracy: 0.9442  Training Loss: 0.1882  Validation Accuracy: 0.9221  Validation Loss: 0.3462\n",
      "Epoch 35  Training Accuracy: 0.9453  Training Loss: 0.1813  Validation Accuracy: 0.9190  Validation Loss: 0.3775\n",
      "Epoch 36  Training Accuracy: 0.9453  Training Loss: 0.1855  Validation Accuracy: 0.9162  Validation Loss: 0.3912\n",
      "Epoch 37  Training Accuracy: 0.9477  Training Loss: 0.1756  Validation Accuracy: 0.9268  Validation Loss: 0.3092\n",
      "Epoch 38  Training Accuracy: 0.9467  Training Loss: 0.1784  Validation Accuracy: 0.9179  Validation Loss: 0.3535\n",
      "Epoch 39  Training Accuracy: 0.9478  Training Loss: 0.1753  Validation Accuracy: 0.9277  Validation Loss: 0.3475\n",
      "Epoch 40  Training Accuracy: 0.9492  Training Loss: 0.1697  Validation Accuracy: 0.9273  Validation Loss: 0.3426\n",
      "Epoch 41  Training Accuracy: 0.9478  Training Loss: 0.1725  Validation Accuracy: 0.9360  Validation Loss: 0.3097\n",
      "Epoch 42  Training Accuracy: 0.9487  Training Loss: 0.1714  Validation Accuracy: 0.9187  Validation Loss: 0.4119\n",
      "Epoch 43  Training Accuracy: 0.9509  Training Loss: 0.1651  Validation Accuracy: 0.9195  Validation Loss: 0.3934\n",
      "Epoch 44  Training Accuracy: 0.9502  Training Loss: 0.1640  Validation Accuracy: 0.9370  Validation Loss: 0.2992\n",
      "Epoch 45  Training Accuracy: 0.9492  Training Loss: 0.1680  Validation Accuracy: 0.9361  Validation Loss: 0.2916\n",
      "Epoch 46  Training Accuracy: 0.9520  Training Loss: 0.1599  Validation Accuracy: 0.9267  Validation Loss: 0.3628\n",
      "Epoch 47  Training Accuracy: 0.9516  Training Loss: 0.1611  Validation Accuracy: 0.9309  Validation Loss: 0.3742\n",
      "Epoch 48  Training Accuracy: 0.9503  Training Loss: 0.1650  Validation Accuracy: 0.9267  Validation Loss: 0.3725\n",
      "Epoch 49  Training Accuracy: 0.9521  Training Loss: 0.1597  Validation Accuracy: 0.9264  Validation Loss: 0.3945\n"
     ]
    }
   ],
   "source": [
    "### Create the network string represetation \n",
    "\n",
    "total_input_size  = 28*28\n",
    "total_ouput_size  = 10\n",
    "dict_network      = dict_network\n",
    "min_amount_layers = 1\n",
    "max_amount_layers = 50\n",
    "drop_layer_rate   = 0.5\n",
    "norm_layer_rate   = 0.5\n",
    "\n",
    "string_model_4 = dict_to_string_representation_network(total_input_size, \n",
    "                                                    total_ouput_size, \n",
    "                                                    dict_network, \n",
    "                                                    min_amount_layers, \n",
    "                                                    max_amount_layers,\n",
    "                                                    drop_layer_rate, \n",
    "                                                    norm_layer_rate)\n",
    "\n",
    "\n",
    "### transfer the network string represetation to a pytorch model \n",
    "\n",
    "model_4 = Net(string_model_4)\n",
    "print(model_4)\n",
    "\n",
    "\n",
    "### Create the optimizer string represetation \n",
    "\n",
    "opt_string_4 = dict_to_string_representation_optimizer(dict_optimizer)\n",
    "\n",
    "\n",
    "### transfer the string represetation to a pytorch optimizer \n",
    "\n",
    "optimizer_4 = parse_opt_string(opt_string_4, model_4)\n",
    "print(optimizer_4)\n",
    "\n",
    "\n",
    "### Train the model \n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_with_validation(model_4, \n",
    "                      train_loader, \n",
    "                      validation_loader, \n",
    "                      loss_fn, \n",
    "                      optimizer_4, \n",
    "                      num_epochs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ab0ae3",
   "metadata": {},
   "source": [
    "#### 6.3.1. Perform - Remove layer mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "f2d8e445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed Layer: 2 AlphaDropout|0.5\n"
     ]
    }
   ],
   "source": [
    "remove_layer_mutation_string = mutation_remove_layer(string_model_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c2b056",
   "metadata": {},
   "source": [
    "#### 6.3.1 Train Network 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "0d24443a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): CELU(alpha=1.0)\n",
      "    (3): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (4): ELU(alpha=1.0)\n",
      "    (5): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (6): BatchNorm1d(128, eps=0.1, momentum=0.5, affine=True, track_running_stats=True)\n",
      "    (7): PReLU(num_parameters=1)\n",
      "    (8): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (9): AlphaDropout(p=0.5, inplace=False)\n",
      "    (10): ReLU()\n",
      "    (11): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (12): BatchNorm1d(512, eps=0.1, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): SiLU()\n",
      "    (14): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (15): AlphaDropout(p=0.1, inplace=False)\n",
      "    (16): CELU(alpha=1.0)\n",
      "    (17): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (18): BatchNorm1d(128, eps=0.01, momentum=0.5, affine=True, track_running_stats=True)\n",
      "    (19): AlphaDropout(p=0.3, inplace=False)\n",
      "    (20): CELU(alpha=1.0)\n",
      "    (21): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
      ")\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.8, 0.9)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0  Training Accuracy: 0.8286  Training Loss: 0.6089  Validation Accuracy: 0.9141  Validation Loss: 0.2725\n",
      "Epoch 1  Training Accuracy: 0.9239  Training Loss: 0.2685  Validation Accuracy: 0.8712  Validation Loss: 0.4207\n",
      "Epoch 2  Training Accuracy: 0.9410  Training Loss: 0.2050  Validation Accuracy: 0.7885  Validation Loss: 0.8763\n",
      "Epoch 3  Training Accuracy: 0.9513  Training Loss: 0.1672  Validation Accuracy: 0.6845  Validation Loss: 1.1954\n",
      "Epoch 4  Training Accuracy: 0.9574  Training Loss: 0.1445  Validation Accuracy: 0.5821  Validation Loss: 1.5141\n",
      "Epoch 5  Training Accuracy: 0.9626  Training Loss: 0.1316  Validation Accuracy: 0.5912  Validation Loss: 1.6387\n",
      "Epoch 6  Training Accuracy: 0.9650  Training Loss: 0.1190  Validation Accuracy: 0.4310  Validation Loss: 2.4116\n",
      "Epoch 7  Training Accuracy: 0.9686  Training Loss: 0.1067  Validation Accuracy: 0.5293  Validation Loss: 2.0766\n",
      "Epoch 8  Training Accuracy: 0.9703  Training Loss: 0.0998  Validation Accuracy: 0.4371  Validation Loss: 2.4632\n",
      "Epoch 9  Training Accuracy: 0.9729  Training Loss: 0.0924  Validation Accuracy: 0.3800  Validation Loss: 2.6313\n",
      "Epoch 10  Training Accuracy: 0.9750  Training Loss: 0.0850  Validation Accuracy: 0.4731  Validation Loss: 2.4410\n",
      "Epoch 11  Training Accuracy: 0.9760  Training Loss: 0.0815  Validation Accuracy: 0.4330  Validation Loss: 2.4476\n",
      "Epoch 12  Training Accuracy: 0.9772  Training Loss: 0.0738  Validation Accuracy: 0.4764  Validation Loss: 2.3973\n",
      "Epoch 13  Training Accuracy: 0.9783  Training Loss: 0.0731  Validation Accuracy: 0.3918  Validation Loss: 2.6394\n",
      "Epoch 14  Training Accuracy: 0.9787  Training Loss: 0.0697  Validation Accuracy: 0.3374  Validation Loss: 2.6093\n",
      "Epoch 15  Training Accuracy: 0.9792  Training Loss: 0.0679  Validation Accuracy: 0.5450  Validation Loss: 1.5958\n",
      "Epoch 16  Training Accuracy: 0.9813  Training Loss: 0.0623  Validation Accuracy: 0.5389  Validation Loss: 1.8409\n",
      "Epoch 17  Training Accuracy: 0.9825  Training Loss: 0.0587  Validation Accuracy: 0.5199  Validation Loss: 1.8287\n",
      "Epoch 18  Training Accuracy: 0.9827  Training Loss: 0.0576  Validation Accuracy: 0.5233  Validation Loss: 1.8511\n",
      "Epoch 19  Training Accuracy: 0.9833  Training Loss: 0.0559  Validation Accuracy: 0.5023  Validation Loss: 1.9526\n",
      "Epoch 20  Training Accuracy: 0.9840  Training Loss: 0.0519  Validation Accuracy: 0.6149  Validation Loss: 1.3592\n",
      "Epoch 21  Training Accuracy: 0.9842  Training Loss: 0.0524  Validation Accuracy: 0.5377  Validation Loss: 1.6432\n",
      "Epoch 22  Training Accuracy: 0.9847  Training Loss: 0.0478  Validation Accuracy: 0.5274  Validation Loss: 1.6612\n",
      "Epoch 23  Training Accuracy: 0.9854  Training Loss: 0.0478  Validation Accuracy: 0.5647  Validation Loss: 1.5276\n",
      "Epoch 24  Training Accuracy: 0.9862  Training Loss: 0.0448  Validation Accuracy: 0.4883  Validation Loss: 1.6212\n",
      "Epoch 25  Training Accuracy: 0.9867  Training Loss: 0.0447  Validation Accuracy: 0.5996  Validation Loss: 1.1993\n",
      "Epoch 26  Training Accuracy: 0.9871  Training Loss: 0.0435  Validation Accuracy: 0.6001  Validation Loss: 1.2260\n",
      "Epoch 27  Training Accuracy: 0.9873  Training Loss: 0.0404  Validation Accuracy: 0.6198  Validation Loss: 1.2032\n",
      "Epoch 28  Training Accuracy: 0.9871  Training Loss: 0.0412  Validation Accuracy: 0.6059  Validation Loss: 1.1790\n",
      "Epoch 29  Training Accuracy: 0.9880  Training Loss: 0.0394  Validation Accuracy: 0.6647  Validation Loss: 1.0141\n",
      "Epoch 30  Training Accuracy: 0.9883  Training Loss: 0.0385  Validation Accuracy: 0.6284  Validation Loss: 1.0805\n",
      "Epoch 31  Training Accuracy: 0.9888  Training Loss: 0.0355  Validation Accuracy: 0.6270  Validation Loss: 1.0639\n",
      "Epoch 32  Training Accuracy: 0.9890  Training Loss: 0.0371  Validation Accuracy: 0.6861  Validation Loss: 0.8807\n",
      "Epoch 33  Training Accuracy: 0.9895  Training Loss: 0.0355  Validation Accuracy: 0.6427  Validation Loss: 1.1159\n",
      "Epoch 34  Training Accuracy: 0.9896  Training Loss: 0.0331  Validation Accuracy: 0.6872  Validation Loss: 0.8991\n",
      "Epoch 35  Training Accuracy: 0.9898  Training Loss: 0.0328  Validation Accuracy: 0.6698  Validation Loss: 0.8730\n",
      "Epoch 36  Training Accuracy: 0.9898  Training Loss: 0.0328  Validation Accuracy: 0.7525  Validation Loss: 0.7130\n",
      "Epoch 37  Training Accuracy: 0.9897  Training Loss: 0.0325  Validation Accuracy: 0.6646  Validation Loss: 0.9659\n",
      "Epoch 38  Training Accuracy: 0.9900  Training Loss: 0.0315  Validation Accuracy: 0.7368  Validation Loss: 0.7616\n",
      "Epoch 39  Training Accuracy: 0.9910  Training Loss: 0.0305  Validation Accuracy: 0.8967  Validation Loss: 0.4121\n",
      "Epoch 40  Training Accuracy: 0.9907  Training Loss: 0.0292  Validation Accuracy: 0.7266  Validation Loss: 0.7568\n",
      "Epoch 41  Training Accuracy: 0.9909  Training Loss: 0.0292  Validation Accuracy: 0.7756  Validation Loss: 0.6422\n",
      "Epoch 42  Training Accuracy: 0.9913  Training Loss: 0.0275  Validation Accuracy: 0.7271  Validation Loss: 0.7402\n",
      "Epoch 43  Training Accuracy: 0.9907  Training Loss: 0.0294  Validation Accuracy: 0.8861  Validation Loss: 0.3999\n",
      "Epoch 44  Training Accuracy: 0.9919  Training Loss: 0.0251  Validation Accuracy: 0.8367  Validation Loss: 0.4754\n",
      "Epoch 45  Training Accuracy: 0.9915  Training Loss: 0.0262  Validation Accuracy: 0.7523  Validation Loss: 0.6816\n",
      "Epoch 46  Training Accuracy: 0.9916  Training Loss: 0.0264  Validation Accuracy: 0.7591  Validation Loss: 0.6464\n",
      "Epoch 47  Training Accuracy: 0.9919  Training Loss: 0.0252  Validation Accuracy: 0.7552  Validation Loss: 0.6515\n",
      "Epoch 48  Training Accuracy: 0.9918  Training Loss: 0.0273  Validation Accuracy: 0.7631  Validation Loss: 0.6178\n",
      "Epoch 49  Training Accuracy: 0.9920  Training Loss: 0.0249  Validation Accuracy: 0.7863  Validation Loss: 0.6173\n"
     ]
    }
   ],
   "source": [
    "### transfer the network string represetation to a pytorch model \n",
    "\n",
    "model_4_2 = Net(remove_layer_mutation_string)\n",
    "print(model_4_2)\n",
    "\n",
    "\n",
    "### transfer the string represetation to a pytorch optimizer \n",
    "\n",
    "optimizer_4_2 = parse_opt_string(opt_string_4, model_4_2)\n",
    "print(optimizer_4_2)\n",
    "\n",
    "\n",
    "### Train the model \n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_with_validation(model_4_2, \n",
    "                      train_loader, \n",
    "                      validation_loader, \n",
    "                      loss_fn, \n",
    "                      optimizer_4_2, \n",
    "                      num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fd2efb",
   "metadata": {},
   "source": [
    "### 6.4.1 Experiment 4 - Network 5 // Change optimizer mutation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14e14dc",
   "metadata": {},
   "source": [
    "#### 6.4.2 Train Network 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "aff8ae33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): AlphaDropout(p=0.7, inplace=False)\n",
      "    (3): PReLU(num_parameters=1)\n",
      "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (8): Dropout(p=0.3, inplace=False)\n",
      "    (9): Sigmoid()\n",
      "    (10): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (11): BatchNorm1d(128, eps=0.001, momentum=0.9, affine=True, track_running_stats=True)\n",
      "    (12): SELU()\n",
      "    (13): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (14): CELU(alpha=1.0)\n",
      "    (15): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (16): AlphaDropout(p=0.1, inplace=False)\n",
      "    (17): SELU()\n",
      "    (18): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (19): BatchNorm1d(512, eps=0.001, momentum=0.9, affine=True, track_running_stats=True)\n",
      "    (20): CELU(alpha=1.0)\n",
      "    (21): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (22): LayerNorm((128,), eps=0.001, elementwise_affine=True)\n",
      "    (23): Dropout(p=0.5, inplace=False)\n",
      "    (24): Sigmoid()\n",
      "    (25): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (26): Dropout(p=0.7, inplace=False)\n",
      "    (27): SiLU()\n",
      "    (28): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (29): Dropout(p=0.5, inplace=False)\n",
      "    (30): ReLU()\n",
      "    (31): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (32): BatchNorm1d(512, eps=0.01, momentum=0.5, affine=True, track_running_stats=True)\n",
      "    (33): Dropout(p=0.1, inplace=False)\n",
      "    (34): SiLU()\n",
      "    (35): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (36): LayerNorm((128,), eps=0.001, elementwise_affine=True)\n",
      "    (37): AlphaDropout(p=0.1, inplace=False)\n",
      "    (38): ELU(alpha=1.0)\n",
      "    (39): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
      ")\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: True\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0  Training Accuracy: 0.0988  Training Loss: 2.4241  Validation Accuracy: 0.0997  Validation Loss: 2.4771\n",
      "Epoch 1  Training Accuracy: 0.0977  Training Loss: 2.4257  Validation Accuracy: 0.1601  Validation Loss: 2.4174\n",
      "Epoch 2  Training Accuracy: 0.0999  Training Loss: 2.4281  Validation Accuracy: 0.1414  Validation Loss: 2.4206\n",
      "Epoch 3  Training Accuracy: 0.0992  Training Loss: 2.4257  Validation Accuracy: 0.1406  Validation Loss: 2.4495\n",
      "Epoch 4  Training Accuracy: 0.0983  Training Loss: 2.4259  Validation Accuracy: 0.1051  Validation Loss: 2.4631\n",
      "Epoch 5  Training Accuracy: 0.1002  Training Loss: 2.4241  Validation Accuracy: 0.1045  Validation Loss: 2.4641\n",
      "Epoch 6  Training Accuracy: 0.0971  Training Loss: 2.4268  Validation Accuracy: 0.1616  Validation Loss: 2.4479\n",
      "Epoch 7  Training Accuracy: 0.0975  Training Loss: 2.4263  Validation Accuracy: 0.1077  Validation Loss: 2.4387\n",
      "Epoch 8  Training Accuracy: 0.0971  Training Loss: 2.4267  Validation Accuracy: 0.1167  Validation Loss: 2.4552\n",
      "Epoch 9  Training Accuracy: 0.1008  Training Loss: 2.4236  Validation Accuracy: 0.1033  Validation Loss: 2.4696\n",
      "Epoch 10  Training Accuracy: 0.1003  Training Loss: 2.4251  Validation Accuracy: 0.1043  Validation Loss: 2.4528\n",
      "Epoch 11  Training Accuracy: 0.0975  Training Loss: 2.4250  Validation Accuracy: 0.1067  Validation Loss: 2.4347\n",
      "Epoch 12  Training Accuracy: 0.1007  Training Loss: 2.4231  Validation Accuracy: 0.1491  Validation Loss: 2.4018\n",
      "Epoch 13  Training Accuracy: 0.1004  Training Loss: 2.4252  Validation Accuracy: 0.1018  Validation Loss: 2.4527\n",
      "Epoch 14  Training Accuracy: 0.0987  Training Loss: 2.4218  Validation Accuracy: 0.1306  Validation Loss: 2.4646\n",
      "Epoch 15  Training Accuracy: 0.1000  Training Loss: 2.4253  Validation Accuracy: 0.1111  Validation Loss: 2.4157\n",
      "Epoch 16  Training Accuracy: 0.0968  Training Loss: 2.4251  Validation Accuracy: 0.1086  Validation Loss: 2.4428\n",
      "Epoch 17  Training Accuracy: 0.0987  Training Loss: 2.4234  Validation Accuracy: 0.1282  Validation Loss: 2.4497\n",
      "Epoch 18  Training Accuracy: 0.1005  Training Loss: 2.4238  Validation Accuracy: 0.1165  Validation Loss: 2.4512\n",
      "Epoch 19  Training Accuracy: 0.1001  Training Loss: 2.4214  Validation Accuracy: 0.1091  Validation Loss: 2.4373\n",
      "Epoch 20  Training Accuracy: 0.1020  Training Loss: 2.4226  Validation Accuracy: 0.1032  Validation Loss: 2.4755\n",
      "Epoch 21  Training Accuracy: 0.1002  Training Loss: 2.4262  Validation Accuracy: 0.0932  Validation Loss: 2.4452\n",
      "Epoch 22  Training Accuracy: 0.0990  Training Loss: 2.4231  Validation Accuracy: 0.1289  Validation Loss: 2.4446\n",
      "Epoch 23  Training Accuracy: 0.0997  Training Loss: 2.4248  Validation Accuracy: 0.1019  Validation Loss: 2.4531\n",
      "Epoch 24  Training Accuracy: 0.0992  Training Loss: 2.4244  Validation Accuracy: 0.1107  Validation Loss: 2.4098\n",
      "Epoch 25  Training Accuracy: 0.0999  Training Loss: 2.4243  Validation Accuracy: 0.1096  Validation Loss: 2.4045\n",
      "Epoch 26  Training Accuracy: 0.0987  Training Loss: 2.4263  Validation Accuracy: 0.0919  Validation Loss: 2.4473\n",
      "Epoch 27  Training Accuracy: 0.1000  Training Loss: 2.4218  Validation Accuracy: 0.1534  Validation Loss: 2.4273\n",
      "Epoch 28  Training Accuracy: 0.0993  Training Loss: 2.4228  Validation Accuracy: 0.1245  Validation Loss: 2.4219\n",
      "Epoch 29  Training Accuracy: 0.0998  Training Loss: 2.4255  Validation Accuracy: 0.1216  Validation Loss: 2.4487\n",
      "Epoch 30  Training Accuracy: 0.0974  Training Loss: 2.4249  Validation Accuracy: 0.1373  Validation Loss: 2.4101\n",
      "Epoch 31  Training Accuracy: 0.0981  Training Loss: 2.4257  Validation Accuracy: 0.1410  Validation Loss: 2.4149\n",
      "Epoch 32  Training Accuracy: 0.0976  Training Loss: 2.4265  Validation Accuracy: 0.1038  Validation Loss: 2.4350\n",
      "Epoch 33  Training Accuracy: 0.0989  Training Loss: 2.4247  Validation Accuracy: 0.0992  Validation Loss: 2.4409\n",
      "Epoch 34  Training Accuracy: 0.1007  Training Loss: 2.4223  Validation Accuracy: 0.1092  Validation Loss: 2.4432\n",
      "Epoch 35  Training Accuracy: 0.0985  Training Loss: 2.4232  Validation Accuracy: 0.1043  Validation Loss: 2.4374\n",
      "Epoch 36  Training Accuracy: 0.0992  Training Loss: 2.4261  Validation Accuracy: 0.1368  Validation Loss: 2.4013\n",
      "Epoch 37  Training Accuracy: 0.0995  Training Loss: 2.4241  Validation Accuracy: 0.1479  Validation Loss: 2.4213\n",
      "Epoch 38  Training Accuracy: 0.0991  Training Loss: 2.4269  Validation Accuracy: 0.1415  Validation Loss: 2.3760\n",
      "Epoch 39  Training Accuracy: 0.0961  Training Loss: 2.4303  Validation Accuracy: 0.1070  Validation Loss: 2.4116\n",
      "Epoch 40  Training Accuracy: 0.0998  Training Loss: 2.4229  Validation Accuracy: 0.1434  Validation Loss: 2.4435\n",
      "Epoch 41  Training Accuracy: 0.1003  Training Loss: 2.4246  Validation Accuracy: 0.1099  Validation Loss: 2.4403\n",
      "Epoch 42  Training Accuracy: 0.0989  Training Loss: 2.4274  Validation Accuracy: 0.1058  Validation Loss: 2.4219\n",
      "Epoch 43  Training Accuracy: 0.0974  Training Loss: 2.4262  Validation Accuracy: 0.1171  Validation Loss: 2.4402\n",
      "Epoch 44  Training Accuracy: 0.0972  Training Loss: 2.4268  Validation Accuracy: 0.1046  Validation Loss: 2.4503\n",
      "Epoch 45  Training Accuracy: 0.0983  Training Loss: 2.4263  Validation Accuracy: 0.1066  Validation Loss: 2.4270\n",
      "Epoch 46  Training Accuracy: 0.0997  Training Loss: 2.4228  Validation Accuracy: 0.1511  Validation Loss: 2.4440\n",
      "Epoch 47  Training Accuracy: 0.0992  Training Loss: 2.4248  Validation Accuracy: 0.1066  Validation Loss: 2.4159\n",
      "Epoch 48  Training Accuracy: 0.0975  Training Loss: 2.4251  Validation Accuracy: 0.1025  Validation Loss: 2.4569\n",
      "Epoch 49  Training Accuracy: 0.0997  Training Loss: 2.4234  Validation Accuracy: 0.1067  Validation Loss: 2.4174\n"
     ]
    }
   ],
   "source": [
    "### Create the network string represetation \n",
    "\n",
    "total_input_size  = 28*28\n",
    "total_ouput_size  = 10\n",
    "dict_network      = dict_network\n",
    "min_amount_layers = 1\n",
    "max_amount_layers = 50\n",
    "drop_layer_rate   = 0.5\n",
    "norm_layer_rate   = 0.5\n",
    "\n",
    "string_model_5 = dict_to_string_representation_network(total_input_size, \n",
    "                                                    total_ouput_size, \n",
    "                                                    dict_network, \n",
    "                                                    min_amount_layers, \n",
    "                                                    max_amount_layers,\n",
    "                                                    drop_layer_rate, \n",
    "                                                    norm_layer_rate)\n",
    "\n",
    "\n",
    "### transfer the network string represetation to a pytorch model \n",
    "\n",
    "model_5 = Net(string_model_5)\n",
    "print(model_5)\n",
    "\n",
    "\n",
    "### Create the optimizer string represetation \n",
    "\n",
    "opt_string_5 = dict_to_string_representation_optimizer(dict_optimizer)\n",
    "\n",
    "\n",
    "### transfer the string represetation to a pytorch optimizer \n",
    "\n",
    "optimizer_5 = parse_opt_string(opt_string_5, model_5)\n",
    "print(optimizer_5)\n",
    "\n",
    "\n",
    "### Train the model \n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_with_validation(model_5, \n",
    "                      train_loader, \n",
    "                      validation_loader, \n",
    "                      loss_fn, \n",
    "                      optimizer_5, \n",
    "                      num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845a8a2e",
   "metadata": {},
   "source": [
    "#### 6.4.2 Perform - Change optimizer mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "7f0f1a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original optimizer string: SGD|0.01;0.9;True\n",
      "parameters in string ['SGD', '0.01', '0.9', 'True']\n",
      "parameter to change: True\n",
      "out of following parameters: [False, True]\n",
      "new parameter: False\n"
     ]
    }
   ],
   "source": [
    "mutated_optimizer_string = mutation_change_opt(opt_string_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f5963d",
   "metadata": {},
   "source": [
    "#### 6.4.3 Train Network 5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "ef97e8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: True\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0  Training Accuracy: 0.1000  Training Loss: 2.4446  Validation Accuracy: 0.0888  Validation Loss: 2.5027\n",
      "Epoch 1  Training Accuracy: 0.1016  Training Loss: 2.4358  Validation Accuracy: 0.1110  Validation Loss: 2.5667\n",
      "Epoch 2  Training Accuracy: 0.0992  Training Loss: 2.4435  Validation Accuracy: 0.1053  Validation Loss: 2.5045\n",
      "Epoch 3  Training Accuracy: 0.1015  Training Loss: 2.4401  Validation Accuracy: 0.0974  Validation Loss: 2.4802\n",
      "Epoch 4  Training Accuracy: 0.1011  Training Loss: 2.4377  Validation Accuracy: 0.1009  Validation Loss: 2.4962\n",
      "Epoch 5  Training Accuracy: 0.1009  Training Loss: 2.4382  Validation Accuracy: 0.0906  Validation Loss: 2.5071\n",
      "Epoch 6  Training Accuracy: 0.1001  Training Loss: 2.4368  Validation Accuracy: 0.1021  Validation Loss: 2.5174\n",
      "Epoch 7  Training Accuracy: 0.1009  Training Loss: 2.4404  Validation Accuracy: 0.1116  Validation Loss: 2.4700\n",
      "Epoch 8  Training Accuracy: 0.1020  Training Loss: 2.4411  Validation Accuracy: 0.1391  Validation Loss: 2.4818\n",
      "Epoch 9  Training Accuracy: 0.1013  Training Loss: 2.4390  Validation Accuracy: 0.1006  Validation Loss: 2.5192\n",
      "Epoch 10  Training Accuracy: 0.1008  Training Loss: 2.4389  Validation Accuracy: 0.0950  Validation Loss: 2.4724\n",
      "Epoch 11  Training Accuracy: 0.0997  Training Loss: 2.4395  Validation Accuracy: 0.1037  Validation Loss: 2.5232\n",
      "Epoch 12  Training Accuracy: 0.1033  Training Loss: 2.4369  Validation Accuracy: 0.1123  Validation Loss: 2.5002\n",
      "Epoch 13  Training Accuracy: 0.0983  Training Loss: 2.4425  Validation Accuracy: 0.1005  Validation Loss: 2.5011\n",
      "Epoch 14  Training Accuracy: 0.1006  Training Loss: 2.4415  Validation Accuracy: 0.0922  Validation Loss: 2.5345\n",
      "Epoch 15  Training Accuracy: 0.1012  Training Loss: 2.4401  Validation Accuracy: 0.0917  Validation Loss: 2.5230\n",
      "Epoch 16  Training Accuracy: 0.1005  Training Loss: 2.4408  Validation Accuracy: 0.1033  Validation Loss: 2.5272\n",
      "Epoch 17  Training Accuracy: 0.1011  Training Loss: 2.4368  Validation Accuracy: 0.0985  Validation Loss: 2.5187\n",
      "Epoch 18  Training Accuracy: 0.1027  Training Loss: 2.4392  Validation Accuracy: 0.1018  Validation Loss: 2.5123\n",
      "Epoch 19  Training Accuracy: 0.1014  Training Loss: 2.4411  Validation Accuracy: 0.0902  Validation Loss: 2.5206\n",
      "Epoch 20  Training Accuracy: 0.1013  Training Loss: 2.4407  Validation Accuracy: 0.1087  Validation Loss: 2.4825\n",
      "Epoch 21  Training Accuracy: 0.1000  Training Loss: 2.4430  Validation Accuracy: 0.1052  Validation Loss: 2.5336\n",
      "Epoch 22  Training Accuracy: 0.1022  Training Loss: 2.4393  Validation Accuracy: 0.0991  Validation Loss: 2.5098\n",
      "Epoch 23  Training Accuracy: 0.1015  Training Loss: 2.4383  Validation Accuracy: 0.0813  Validation Loss: 2.5201\n",
      "Epoch 24  Training Accuracy: 0.1009  Training Loss: 2.4417  Validation Accuracy: 0.0861  Validation Loss: 2.4570\n",
      "Epoch 25  Training Accuracy: 0.1016  Training Loss: 2.4394  Validation Accuracy: 0.1037  Validation Loss: 2.4960\n",
      "Epoch 26  Training Accuracy: 0.1019  Training Loss: 2.4404  Validation Accuracy: 0.0899  Validation Loss: 2.4670\n",
      "Epoch 27  Training Accuracy: 0.1006  Training Loss: 2.4413  Validation Accuracy: 0.1183  Validation Loss: 2.4631\n",
      "Epoch 28  Training Accuracy: 0.0982  Training Loss: 2.4440  Validation Accuracy: 0.1156  Validation Loss: 2.4856\n",
      "Epoch 29  Training Accuracy: 0.1003  Training Loss: 2.4392  Validation Accuracy: 0.1146  Validation Loss: 2.4903\n",
      "Epoch 30  Training Accuracy: 0.1007  Training Loss: 2.4405  Validation Accuracy: 0.1017  Validation Loss: 2.4751\n",
      "Epoch 31  Training Accuracy: 0.1006  Training Loss: 2.4407  Validation Accuracy: 0.1009  Validation Loss: 2.4695\n",
      "Epoch 32  Training Accuracy: 0.1009  Training Loss: 2.4416  Validation Accuracy: 0.1038  Validation Loss: 2.5042\n",
      "Epoch 33  Training Accuracy: 0.1001  Training Loss: 2.4406  Validation Accuracy: 0.1009  Validation Loss: 2.5185\n",
      "Epoch 34  Training Accuracy: 0.1009  Training Loss: 2.4408  Validation Accuracy: 0.0994  Validation Loss: 2.4998\n",
      "Epoch 35  Training Accuracy: 0.0997  Training Loss: 2.4394  Validation Accuracy: 0.0936  Validation Loss: 2.5332\n",
      "Epoch 36  Training Accuracy: 0.1007  Training Loss: 2.4400  Validation Accuracy: 0.1018  Validation Loss: 2.4819\n",
      "Epoch 37  Training Accuracy: 0.1016  Training Loss: 2.4402  Validation Accuracy: 0.0846  Validation Loss: 2.5115\n",
      "Epoch 38  Training Accuracy: 0.0991  Training Loss: 2.4441  Validation Accuracy: 0.1097  Validation Loss: 2.4998\n",
      "Epoch 39  Training Accuracy: 0.0991  Training Loss: 2.4409  Validation Accuracy: 0.1203  Validation Loss: 2.5112\n",
      "Epoch 40  Training Accuracy: 0.1025  Training Loss: 2.4396  Validation Accuracy: 0.0987  Validation Loss: 2.4598\n",
      "Epoch 41  Training Accuracy: 0.0998  Training Loss: 2.4431  Validation Accuracy: 0.1100  Validation Loss: 2.5107\n",
      "Epoch 42  Training Accuracy: 0.0986  Training Loss: 2.4446  Validation Accuracy: 0.1042  Validation Loss: 2.5063\n",
      "Epoch 43  Training Accuracy: 0.0986  Training Loss: 2.4410  Validation Accuracy: 0.0694  Validation Loss: 2.4946\n",
      "Epoch 44  Training Accuracy: 0.0989  Training Loss: 2.4441  Validation Accuracy: 0.1031  Validation Loss: 2.4839\n",
      "Epoch 45  Training Accuracy: 0.0993  Training Loss: 2.4441  Validation Accuracy: 0.1144  Validation Loss: 2.4813\n",
      "Epoch 46  Training Accuracy: 0.1002  Training Loss: 2.4397  Validation Accuracy: 0.0903  Validation Loss: 2.4544\n",
      "Epoch 47  Training Accuracy: 0.0996  Training Loss: 2.4422  Validation Accuracy: 0.0863  Validation Loss: 2.5040\n",
      "Epoch 48  Training Accuracy: 0.0999  Training Loss: 2.4416  Validation Accuracy: 0.0758  Validation Loss: 2.5238\n",
      "Epoch 49  Training Accuracy: 0.1009  Training Loss: 2.4408  Validation Accuracy: 0.1044  Validation Loss: 2.5135\n"
     ]
    }
   ],
   "source": [
    "### transfer the string represetation to a pytorch optimizer \n",
    "\n",
    "optimizer_5_2 = parse_opt_string(mutated_optimizer_string, model_5)\n",
    "print(optimizer_5_2)\n",
    "\n",
    "\n",
    "### Train the model \n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_with_validation(model_5, \n",
    "                      train_loader, \n",
    "                      validation_loader, \n",
    "                      loss_fn, \n",
    "                      optimizer_5_2, \n",
    "                      num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
